{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12178530,"sourceType":"datasetVersion","datasetId":7667745},{"sourceId":12258871,"sourceType":"datasetVersion","datasetId":7724798},{"sourceId":12260115,"sourceType":"datasetVersion","datasetId":7725668},{"sourceId":12260118,"sourceType":"datasetVersion","datasetId":7725671},{"sourceId":12260132,"sourceType":"datasetVersion","datasetId":7725682},{"sourceId":12260140,"sourceType":"datasetVersion","datasetId":7725687},{"sourceId":12260915,"sourceType":"datasetVersion","datasetId":7726215},{"sourceId":12261377,"sourceType":"datasetVersion","datasetId":7726520},{"sourceId":12261380,"sourceType":"datasetVersion","datasetId":7726522},{"sourceId":12261384,"sourceType":"datasetVersion","datasetId":7726526},{"sourceId":12261656,"sourceType":"datasetVersion","datasetId":7726656},{"sourceId":12261747,"sourceType":"datasetVersion","datasetId":7726704},{"sourceId":12261881,"sourceType":"datasetVersion","datasetId":7726771},{"sourceId":12261987,"sourceType":"datasetVersion","datasetId":7726829},{"sourceId":12262089,"sourceType":"datasetVersion","datasetId":7726871},{"sourceId":12262209,"sourceType":"datasetVersion","datasetId":7726951},{"sourceId":12262511,"sourceType":"datasetVersion","datasetId":7727149},{"sourceId":12262969,"sourceType":"datasetVersion","datasetId":7727445},{"sourceId":12262977,"sourceType":"datasetVersion","datasetId":7727449},{"sourceId":12262990,"sourceType":"datasetVersion","datasetId":7727460},{"sourceId":12263485,"sourceType":"datasetVersion","datasetId":7727795},{"sourceId":12263700,"sourceType":"datasetVersion","datasetId":7727943},{"sourceId":12263958,"sourceType":"datasetVersion","datasetId":7728115},{"sourceId":12264237,"sourceType":"datasetVersion","datasetId":7728313},{"sourceId":12264568,"sourceType":"datasetVersion","datasetId":7728549},{"sourceId":12266296,"sourceType":"datasetVersion","datasetId":7729656},{"sourceId":12266609,"sourceType":"datasetVersion","datasetId":7729885},{"sourceId":12267002,"sourceType":"datasetVersion","datasetId":7730094},{"sourceId":12267385,"sourceType":"datasetVersion","datasetId":7730298},{"sourceId":12267780,"sourceType":"datasetVersion","datasetId":7730512},{"sourceId":12268143,"sourceType":"datasetVersion","datasetId":7730748},{"sourceId":12268829,"sourceType":"datasetVersion","datasetId":7731234},{"sourceId":12269161,"sourceType":"datasetVersion","datasetId":7731476},{"sourceId":12269508,"sourceType":"datasetVersion","datasetId":7731717},{"sourceId":12269900,"sourceType":"datasetVersion","datasetId":7731992},{"sourceId":12270203,"sourceType":"datasetVersion","datasetId":7732211},{"sourceId":12270524,"sourceType":"datasetVersion","datasetId":7732444},{"sourceId":12270878,"sourceType":"datasetVersion","datasetId":7732697},{"sourceId":12271237,"sourceType":"datasetVersion","datasetId":7732951},{"sourceId":12271536,"sourceType":"datasetVersion","datasetId":7733171},{"sourceId":12272676,"sourceType":"datasetVersion","datasetId":7733995},{"sourceId":12272922,"sourceType":"datasetVersion","datasetId":7734139},{"sourceId":12273187,"sourceType":"datasetVersion","datasetId":7734271},{"sourceId":12273351,"sourceType":"datasetVersion","datasetId":7734362},{"sourceId":12273663,"sourceType":"datasetVersion","datasetId":7734580},{"sourceId":12273807,"sourceType":"datasetVersion","datasetId":7734662},{"sourceId":12273954,"sourceType":"datasetVersion","datasetId":7734744},{"sourceId":12274094,"sourceType":"datasetVersion","datasetId":7734821},{"sourceId":12274232,"sourceType":"datasetVersion","datasetId":7734887},{"sourceId":12274450,"sourceType":"datasetVersion","datasetId":7735015},{"sourceId":12274596,"sourceType":"datasetVersion","datasetId":7735093},{"sourceId":12274759,"sourceType":"datasetVersion","datasetId":7735193},{"sourceId":12274923,"sourceType":"datasetVersion","datasetId":7735316},{"sourceId":12275109,"sourceType":"datasetVersion","datasetId":7735439},{"sourceId":12275375,"sourceType":"datasetVersion","datasetId":7735616},{"sourceId":12275615,"sourceType":"datasetVersion","datasetId":7735786},{"sourceId":12275962,"sourceType":"datasetVersion","datasetId":7736002},{"sourceId":12276351,"sourceType":"datasetVersion","datasetId":7736256},{"sourceId":12276835,"sourceType":"datasetVersion","datasetId":7736603},{"sourceId":12277031,"sourceType":"datasetVersion","datasetId":7736746},{"sourceId":12292652,"sourceType":"datasetVersion","datasetId":7747614},{"sourceId":12301073,"sourceType":"datasetVersion","datasetId":7753343},{"sourceId":12315300,"sourceType":"datasetVersion","datasetId":7759220,"isSourceIdPinned":true},{"sourceId":12320104,"sourceType":"datasetVersion","datasetId":7765705},{"sourceId":12328694,"sourceType":"datasetVersion","datasetId":7771570},{"sourceId":12335239,"sourceType":"datasetVersion","datasetId":7775840}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":37871.770514,"end_time":"2025-06-29T10:17:23.073100","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-28T23:46:11.302586","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"586a8aa7","cell_type":"code","source":"import shutil\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport torch\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\nimport random\nimport tensorflow as tf\nimport torchvision.transforms.functional as TransformerF\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nimport math\nfrom copy import deepcopy\nimport json\nimport time\n\n","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:00.701440Z","iopub.execute_input":"2025-07-01T05:43:00.701694Z","iopub.status.idle":"2025-07-01T05:43:07.752434Z","shell.execute_reply.started":"2025-07-01T05:43:00.701669Z","shell.execute_reply":"2025-07-01T05:43:07.751863Z"},"papermill":{"duration":24.539138,"end_time":"2025-06-28T23:46:39.937168","exception":false,"start_time":"2025-06-28T23:46:15.398030","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2025-07-01 05:43:02.755119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751348582.778083     235 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751348582.785308     235 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"805bd58b","cell_type":"code","source":"class ImageCropper():\n  def __init__(self,debuging,target_size = (512, 512) ):\n    self.target_size=target_size\n    self.margin=20\n    self.debuging= debuging\n    self.transform=None\n    self.normalized=False\n    if debuging:\n      self.cropped_image = None\n\n  def cropImage(self,image, landmarks, checker):\n    if torch.is_tensor(image):\n        image = image.cpu().numpy()\n        image = (image).astype(np.uint8)\n    if torch.is_tensor(landmarks):\n        landmarks_np = landmarks.cpu().numpy()\n    else:\n        landmarks_np = np.array(landmarks).copy()\n\n    # Ensure landmarks shape is (N, 2)\n    if landmarks_np.ndim == 1 and landmarks_np.shape[0] % 2 == 0:\n        landmarks_np = landmarks_np.reshape(-1, 2)\n    elif landmarks_np.ndim != 2 or landmarks_np.shape[1] != 2:\n        raise ValueError(f\"Invalid landmarks shape: {landmarks_np.shape}\")\n    \n    frame = image\n    lm = landmarks_np\n    if not checker['face_landmarks']:\n        variable = lm[6+21]\n        variable[1]-=50\n        lm[54:532,:] = variable\n        \n    \n    if not checker['right_hand_landmarks']:\n        lm[:21,:] = lm[17+21]\n    \n    if not checker['left_hand_landmarks']:\n        lm[532:,:] = lm[18+21]\n    # print(\"this in cropped image\")\n    # for i in range(len(lm[21:54,:])):\n    #       print(\"index: \",i,\") pose: \",lm[21:54,:][i])\n    \n    cropped_image, (x_min, x_max, y_min, y_max) = self.detect_and_crop_person(frame, lm, margin=self.margin)\n    if cropped_image.size == 0:\n        adjusted_landmarks = np.zeros_like(landmarks_np)\n        output_image = torch.zeros(3, self.target_size[1], self.target_size[0])\n        return output_image, adjusted_landmarks\n\n    processed_img, scale, (x_offset, y_offset) = self.resize_and_pad(cropped_image, target_size=self.target_size)\n\n    if self.normalized:\n        h, w, _ = frame.shape\n        x_coords = lm[:, 0] * w\n        y_coords = lm[:, 1] * h\n    else:\n        x_coords = lm[:, 0]\n        y_coords = lm[:, 1]\n\n    adjusted_x = (x_coords - x_min) * scale + x_offset\n    adjusted_y = (y_coords - y_min) * scale + y_offset\n\n    target_w, target_h = self.target_size\n    # out_of_bounds = (\n    #     (adjusted_x < 0) | (adjusted_x >= target_w) |\n    #     (adjusted_y < 0) | (adjusted_y >= target_h)\n    # )\n    # adjusted_x[out_of_bounds] = 0.0\n    # adjusted_y[out_of_bounds] = 0.0\n\n    if self.normalized:\n        adjusted_x /= target_w\n        adjusted_y /= target_h\n\n    adjusted_landmarks = np.stack([adjusted_x, adjusted_y], axis=-1)\n\n    if self.transform:\n        processed_img = self.transform(processed_img)\n    else:\n        processed_img = torch.from_numpy(processed_img).permute(2, 0, 1).float()\n    if self.debuging:\n      self.cropped_image = processed_img\n    return processed_img, adjusted_landmarks\n\n  def resize_and_pad(self,image, target_size=(512, 512)):\n      target_w, target_h = target_size\n      h, w, _ = image.shape\n      if h == 0 or w == 0:\n          return np.zeros((target_h, target_w, 3), dtype=np.uint8), 1.0, (0, 0)\n      padded_image = np.zeros((target_h, target_w, 3), dtype=np.uint8)\n      scale = min(target_w / w, target_h / h)\n      new_w, new_h = int(w * scale), int(h * scale)\n      if new_w == 0 or new_h == 0:\n          return padded_image, 1.0, (0, 0)\n      resized_image = cv2.resize(image, (new_w, new_h))\n      x_offset = (target_w - new_w) // 2\n      y_offset = (target_h - new_h) // 2\n      padded_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_image\n      return padded_image, scale, (x_offset, y_offset)\n\n  def detect_and_crop_person(self,frame, lm, margin=10):\n      h, w, _ = frame.shape\n      if self.normalized:\n          x_coords = lm[:, 0] * w\n          y_coords = lm[:, 1] * h\n      else:\n          x_coords = lm[:, 0]\n          y_coords = lm[:, 1]\n      x_min = int(max(np.min(x_coords) - margin, 0))\n      x_max = int(min(np.max(x_coords) + margin, w))\n      y_min = int(max(np.min(y_coords) - margin, 0))\n      y_max = int(min(np.max(y_coords) + margin, h))\n      if x_max <= x_min or y_max <= y_min:\n          return frame, (0, w, 0, h)\n      crop = frame[y_min:y_max, x_min:x_max]\n      if crop.size == 0:\n          return frame, (0, w, 0, h)\n      return crop, (x_min, x_max, y_min, y_max)\n  def showImage(self):\n    print(\"this cropped image: \")\n    image = self.cropped_image.permute(1,2,0).numpy().astype(dtype=np.uint16)\n    plt.imshow(image)\n    plt.show()\n\nclass FeaturesExtractorFromImage():\n  def __init__(self,debuging):\n    if debuging:\n      self.info = {\n          \"left_hand_image\":None,\n          \"right_hand_image\":None,\n          \"vertical\":None,\n          \"horizontal\":None,\n          \"left_distance\":None,\n          \"right_distance\":None\n      }\n\n    self.debuging= debuging\n    pass\n\n  def extractFeatures(self,image, landmarks_checker, scaled_landmarks):\n\n    if not landmarks_checker[\"pose_landmarks\"] and not landmarks_checker[\"right_hand_landmarks\"] and not landmarks_checker[\"left_hand_landmarks\"] and not landmarks_checker[\"face_landmarks\"]:\n      return list(np.zeros((39514)))\n    scaled_landmarks = scaled_landmarks\n    # right hands landmarks:  21\n    # pose landmarks:  33\n    # face landmarks:  478\n    # left hands landmarks:  21\n    right_hand_landmarks,pose_landmarks,face_landmarks,left_hand_landmarks = scaled_landmarks[:21,:],scaled_landmarks[21:54,:],scaled_landmarks[54:532,:],scaled_landmarks[532:,:]\n    # print(\"this now in mediapipe\")\n    # for i in range(len(scaled_landmarks[21:54,:])):\n    #       print(\"index: \",i,\") pose: \",scaled_landmarks[21:54,:][i])\n    \n    vertical_regoins, horizontal_regions, right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations =self.handsRegoins(\n      landmarks_checker,\n      right_hand_landmarks,\n      pose_landmarks,\n      face_landmarks,\n      left_hand_landmarks,\n      image\n      )\n\n    left_distance,right_distance,left_hand_image, right_hand_image = self.getHandsDistanceFromFace(\n      landmarks_checker,\n      right_hands_box_cordinations,\n      pose_box_cordinations,\n      face_box_cordinations,\n      left_hands_box_cordinations,\n      image\n      )\n\n    LR_hand_regions = np.array([vertical_regoins,horizontal_regions])\n\n    # print(left_hand_image.reshape(-1).shape)\n    # print(right_hand_image.reshape(-1).shape)\n    # print(LR_hand_regions.reshape(-1).shape)\n    # print(left_distance.reshape(-1).shape)\n    # print(right_distance.reshape(-1).shape)\n    # print(right_hand_landmarks.reshape(-1).shape)\n    # print(pose_landmarks.reshape(-1).shape)\n    # print(face_landmarks.reshape(-1).shape)\n    # print(left_hand_landmarks.reshape(-1).shape)\n\n    # print(\"maximum inside extract functino: \",left_hand_image.max())\n    # plt.imshow( left_hand_image)\n    # plt.show()\n\n\n    frame_features = np.concatenate([left_hand_image.reshape(-1),right_hand_image.reshape(-1),LR_hand_regions.reshape(-1),left_distance.reshape(-1),right_distance.reshape(-1), right_hand_landmarks.reshape(-1),pose_landmarks.reshape(-1),face_landmarks.reshape(-1),left_hand_landmarks.reshape(-1)])\n    return torch.tensor(frame_features)\n\n  def getHandsDistanceFromFace(self,landmarks_checker,right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations, image):\n      margin = 10\n\n      left_hand_image= image[left_hands_box_cordinations[0][1]-margin:left_hands_box_cordinations[1][1]+margin,\n                        left_hands_box_cordinations[0][0]-margin:left_hands_box_cordinations[1][0]+margin]\n\n      right_hand_image= image[right_hands_box_cordinations[0][1]-margin:right_hands_box_cordinations[1][1]+margin,\n                        right_hands_box_cordinations[0][0]-margin:right_hands_box_cordinations[1][0]+margin]\n      left_hand_image = cv2.resize(left_hand_image, (80,80)) if left_hand_image.shape[0] > 0 and left_hand_image.shape[1] > 0 else np.zeros((80,80,3))\n      right_hand_image = cv2.resize(right_hand_image, (80,80)) if right_hand_image.shape[0] > 0 and right_hand_image.shape[1] > 0 else np.zeros((80,80,3))\n      distance_between_left_face = np.array([np.abs(face_box_cordinations[0][0]-left_hands_box_cordinations[0][0]),np.abs(face_box_cordinations[1][1] -left_hands_box_cordinations[1][1])])\n      distance_between_right_face = np.array([np.abs(face_box_cordinations[0][0]-right_hands_box_cordinations[0][0]),np.abs(face_box_cordinations[1][1] -right_hands_box_cordinations[1][1])])\n      if self.debuging:\n        self.info['left_distance'] = distance_between_left_face\n        self.info['right_distance'] = distance_between_right_face\n        self.info['left_hand_image'] = left_hand_image\n        self.info['right_hand_image'] = right_hand_image\n      return distance_between_left_face,distance_between_right_face,(left_hand_image),(right_hand_image)\n\n\n  def handsRegoins(self, landmarks_checker, right_hand_landmarks,pose_landmarks,face_landmarks,left_hand_landmarks,image):\n    right_hands_box_cordinations = np.min(right_hand_landmarks,axis = 0).astype(int), np.max(right_hand_landmarks,axis=0).astype(int)#min bottom left - max top right\n    pose_box_cordinations = np.min(pose_landmarks,axis = 0).astype(int), np.max(pose_landmarks,axis=0).astype(int)#min bottom left - max top right\n    face_box_cordinations = np.min(face_landmarks,axis = 0).astype(int), np.max(face_landmarks,axis=0).astype(int)#min bottom left - max top right\n    left_hands_box_cordinations = np.min(left_hand_landmarks,axis = 0).astype(int), np.max(left_hand_landmarks,axis=0).astype(int)#min bottom left - max top right\n\n    if not landmarks_checker['face_landmarks']:\n      new_box_cordinations = np.array([pose_landmarks[8],pose_landmarks[7],pose_landmarks[10],pose_landmarks[9]])\n      face_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n        \n    if not landmarks_checker[\"right_hand_landmarks\"]:\n      new_box_cordinations = np.array([pose_landmarks[21],pose_landmarks[19],pose_landmarks[15],pose_landmarks[17]])\n      right_hands_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n    if not landmarks_checker[\"left_hand_landmarks\"]:\n      new_box_cordinations = np.array([pose_landmarks[20],pose_landmarks[22],pose_landmarks[18],pose_landmarks[16]])\n      left_hands_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n\n    vertical_hands_locations =[]\n    for hand in [left_hands_box_cordinations,right_hands_box_cordinations]:\n      if hand[0][1] <=face_box_cordinations[0][1]:\n        vertical_hands_locations.append(1)\n      elif hand[0][1] >=face_box_cordinations[0][1] and hand[0][1] < pose_box_cordinations[0][1]:\n        vertical_hands_locations.append(2)\n      elif hand[0][1] <face_box_cordinations[1][1] and hand[0][1] >= pose_box_cordinations[0][1]:\n        vertical_hands_locations.append(3)\n      elif hand[0][1] >pose_box_cordinations[0][1] and hand[0][1]>= face_box_cordinations[1][1] and hand[0][1]< (pose_box_cordinations[0][1]/2)+pose_box_cordinations[0][1] :\n        vertical_hands_locations.append(4)\n      else:\n        vertical_hands_locations.append(5)\n    if self.debuging:\n      print(\"this in mediapipe\")\n      m = image.copy()\n      top_left, bottom_right = left_hands_box_cordinations\n      cx = int((top_left[0] + bottom_right[0]) / 2)\n      cy = int((top_left[1] + bottom_right[1]) / 2)\n      center = (cx, cy)\n      cv2.circle(m, center, radius=5, color=(0, 0, 255), thickness=-1)\n      cv2.rectangle(m, tuple(face_box_cordinations[0]), tuple(face_box_cordinations[1]), (255, 0, 0), 2)\n      cv2.rectangle(m, tuple(left_hands_box_cordinations[0]), tuple(left_hands_box_cordinations[1]), (255, 0, 255), 2)\n      cv2.rectangle(m, tuple(right_hands_box_cordinations[0]), tuple(right_hands_box_cordinations[1]), (255, 255, 0), 2)\n      cv2.rectangle(m, tuple(pose_box_cordinations[0]), tuple(pose_box_cordinations[1]), (255, 255, 255), 2)\n      print(\"this is tuple: \",tuple(pose_box_cordinations[0]), tuple(pose_box_cordinations[1]))\n      # for i in range(len(pose_landmarks)):\n      #     print(\"index: \",i,\") pose: \",pose_landmarks[i])\n      plt.imshow(m)\n      plt.show()\n\n\n    horizontal_hands_locations = []\n    for hand in [left_hands_box_cordinations,right_hands_box_cordinations]:\n      if hand[0][0]< pose_box_cordinations[0][0]:\n        horizontal_hands_locations.append(1)\n      elif hand[0][0] >= pose_box_cordinations[0][0] and hand[0][0] < face_box_cordinations[0][0]:\n        horizontal_hands_locations.append(2)\n      elif hand[0][0] >= face_box_cordinations[0][0] and hand[0][0] < face_box_cordinations[1][0]:\n        horizontal_hands_locations.append(3)\n      elif hand[0][0] >=face_box_cordinations[1][0] and hand[0][0]< pose_box_cordinations[1][0]:\n        horizontal_hands_locations.append(4)\n      else:\n        horizontal_hands_locations.append(5)\n    if self.debuging:\n      self.info['horizontal'] = horizontal_hands_locations\n      self.info['vertical'] = vertical_hands_locations\n    return vertical_hands_locations,horizontal_hands_locations, right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations\n\n  def showFeatures(self):\n    print(\"left hand distance from the face: \", self.info['left_distance'])\n    print(\"right hand distance from the face: \", self.info['right_distance'])\n    print(\"vertical hands regions: \",self.info[\"vertical\"])\n    print(\"horizontal hands regions: \",self.info[\"horizontal\"])\n    print(\"left hand image: \")\n    plt.imshow(self.info[\"left_hand_image\"])\n    plt.show()\n    print(\"right hand image: \")\n    plt.imshow(self.info[\"right_hand_image\"])\n    plt.show()\n\nclass MediapipeFeaturesExtractor():\n  def __init__(self,debuging =True,target_size = (512,512)):      \n    self.imageCropper = ImageCropper(debuging,target_size )\n    self.featuresExtractorFromImage = FeaturesExtractorFromImage(debuging)\n    self.debuging = debuging\n    \n\n  def extractFeaturesFromVideo(self, frames, landmarks, segmented_frames):\n    cropped_images = []\n    mediapipeFeatures = []\n    previous_scaled_up_right_hand_landmarks = [0]\n    previous_scaled_up_pose_landmarks = [0]\n    previous_scaled_up_face_landmarks = [0]\n    previous_scaled_up_left_hand_landmarks = [0]\n\n    # for frame in tqdm(frames):\n    #   self.extractor.extractLandmarks(frame)\n    #   checkers.append(self.extractor.check())\n    #   fframes.append(self.extractor.image)\n    #   landmarks.append(self.extractor.get_x_y_landmarks_scaled_up())\n    for frame,landmark,segmented_frame in zip(frames,landmarks, segmented_frames):\n      \n      checker = {\n          \"right_hand_landmarks\":(landmark[-2][0]==1),\n          \"left_hand_landmarks\":(landmark[-1][1]==1),\n          \"pose_landmarks\":(landmark[-2][1]==1),\n          \"face_landmarks\":(landmark[-1][0]==1),\n      }\n      if not checker['right_hand_landmarks'] and np.sum(previous_scaled_up_right_hand_landmarks) != 0 :\n        landmark[:21,:] = previous_scaled_up_right_hand_landmarks\n\n      if not checker['left_hand_landmarks'] and np.sum(previous_scaled_up_right_hand_landmarks) != 0 :\n        landmark[532:-2,:] = previous_scaled_up_left_hand_landmarks\n\n      if not checker['pose_landmarks'] and np.sum(previous_scaled_up_pose_landmarks) != 0:\n        landmark[21:54,:]= previous_scaled_up_pose_landmarks\n      if not checker['face_landmarks'] and np.sum(previous_scaled_up_face_landmarks) != 0:\n        landmark[54:532,:]= previous_scaled_up_face_landmarks\n      \n      previous_scaled_up_right_hand_landmarks = landmark[:21,:]\n      previous_scaled_up_pose_landmarks = landmark[21:54,:]\n      previous_scaled_up_face_landmarks = landmark[54:532,:]\n      previous_scaled_up_left_hand_landmarks = landmark[532:-2,:]\n      \n      cropped_image, _ = self.imageCropper.cropImage(segmented_frame, landmark[:-2], checker)\n      new_frame, adjusted_landmark = self.imageCropper.cropImage(frame, landmark[:-2], checker)\n      mediapipeFeature = self.featuresExtractorFromImage.extractFeatures(new_frame.permute(1,2,0).numpy().astype(dtype=np.uint16),checker,adjusted_landmark)\n      cropped_images.append(cropped_image)\n      mediapipeFeatures.append(mediapipeFeature)\n      if self.debuging:\n        self.showImages()\n    return torch.stack(cropped_images), torch.stack(mediapipeFeatures)\n\n  def showImages(self):\n    if self.debuging:\n      self.imageCropper.showImage()\n      self.featuresExtractorFromImage.showFeatures()\n    else:\n      print(\"sett debuging flag to True\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.753183Z","iopub.execute_input":"2025-07-01T05:43:07.753619Z","iopub.status.idle":"2025-07-01T05:43:07.792472Z","shell.execute_reply.started":"2025-07-01T05:43:07.753590Z","shell.execute_reply":"2025-07-01T05:43:07.791681Z"},"papermill":{"duration":0.046481,"end_time":"2025-06-28T23:46:39.990056","exception":false,"start_time":"2025-06-28T23:46:39.943575","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"a99fcec2","cell_type":"code","source":"# video = torch.load(\"/kaggle/input/where-gloss1/signer0_sample1026/video.pt\",weights_only=False)\n# landmarks = torch.load(\"/kaggle/input/where-gloss1/signer0_sample1026/landmarks.pt\",weights_only=False)\n# segmented_video = torch.load(\"/kaggle/input/where-gloss1/signer0_sample1026/segmented_video.pt\",weights_only=False)","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.793341Z","iopub.execute_input":"2025-07-01T05:43:07.794105Z","iopub.status.idle":"2025-07-01T05:43:07.813408Z","shell.execute_reply.started":"2025-07-01T05:43:07.794078Z","shell.execute_reply":"2025-07-01T05:43:07.812780Z"},"papermill":{"duration":0.010418,"end_time":"2025-06-28T23:46:40.006388","exception":false,"start_time":"2025-06-28T23:46:39.995970","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"id":"242bb5fb","cell_type":"code","source":"# plt.imshow(video[0]/255)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.814154Z","iopub.execute_input":"2025-07-01T05:43:07.814410Z","iopub.status.idle":"2025-07-01T05:43:07.828948Z","shell.execute_reply.started":"2025-07-01T05:43:07.814386Z","shell.execute_reply":"2025-07-01T05:43:07.828293Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.010117,"end_time":"2025-06-28T23:46:40.022165","exception":false,"start_time":"2025-06-28T23:46:40.012048","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"aa868fb4","cell_type":"code","source":"# mediapipeFeaturesExtractor = MediapipeFeaturesExtractor(debuging=True,target_size=(512,512))\n# cropped_images, mediapipeFeatures = mediapipeFeaturesExtractor.extractFeaturesFromVideo(np.array(video),np.array(landmarks),np.array(segmented_video))","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.829758Z","iopub.execute_input":"2025-07-01T05:43:07.830049Z","iopub.status.idle":"2025-07-01T05:43:07.842856Z","shell.execute_reply.started":"2025-07-01T05:43:07.830033Z","shell.execute_reply":"2025-07-01T05:43:07.842313Z"},"papermill":{"duration":0.010667,"end_time":"2025-06-28T23:46:40.038383","exception":false,"start_time":"2025-06-28T23:46:40.027716","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"ef1cc339","cell_type":"code","source":"# for i in mediapipeFeatures:\n#     plt.imshow(i[0:19200].reshape(80,80,3)/255)\n#     plt.show()\n# train_dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.844757Z","iopub.execute_input":"2025-07-01T05:43:07.844952Z","iopub.status.idle":"2025-07-01T05:43:07.857677Z","shell.execute_reply.started":"2025-07-01T05:43:07.844938Z","shell.execute_reply":"2025-07-01T05:43:07.856956Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.009936,"end_time":"2025-06-28T23:46:40.053909","exception":false,"start_time":"2025-06-28T23:46:40.043973","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"id":"23dce4b1","cell_type":"code","source":"# for cropped_image in cropped_images.permute(0,2,3,1)/255:\n#     plt.imshow(cropped_image)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.858399Z","iopub.execute_input":"2025-07-01T05:43:07.858599Z","iopub.status.idle":"2025-07-01T05:43:07.873694Z","shell.execute_reply.started":"2025-07-01T05:43:07.858584Z","shell.execute_reply":"2025-07-01T05:43:07.873009Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.009574,"end_time":"2025-06-28T23:46:40.069127","exception":false,"start_time":"2025-06-28T23:46:40.059553","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"id":"b2edfc8f","cell_type":"code","source":"class VideoDataset(Dataset):\n  def __init__(self,videos_pathes,labels ,agumentation=False,temp_agumentation=False,RGB_only = False,getCropedImageToo= False , cropedtargetsize=(512, 512)):\n    \n    videos,labels = videos_pathes, labels\n   \n    if RGB_only:\n        new_videos = []\n        new_labels = []\n        for index in range(len(videos)):\n            # print(videos[index],videos[index].split('/')[-1].split('_')[-1])\n            if videos[index].split('/')[-1].split('_')[-1].strip() == 'color.pt':\n                new_videos.append(videos[index])\n                new_labels.append(labels[index])\n        videos = new_videos\n        labels = new_labels\n   \n      \n    self.videos = videos\n    labels = labels\n\n    unique_labels = list(set(labels))\n    unique_labels.sort()\n    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n    self.label_to_index = label_to_index\n    labels = torch.tensor([label_to_index[label] for label in labels])\n    one_hot_encoded_labels = F.one_hot(labels, num_classes=len(unique_labels))\n    self.labels = one_hot_encoded_labels\n    self.agumentation = agumentation\n    self.temp_agumentation = temp_agumentation\n    self.cropedtargetsize = cropedtargetsize\n    self.getCropedImageToo =getCropedImageToo\n\n    self.mediapipeFeaturesExtractor = MediapipeFeaturesExtractor(debuging=False,target_size=(512,512))\n\n  def __len__(self):\n    return len(self.videos)\n  \n    \n  def __getitem__(self,index):    \n      landmarks = torch.load(self.videos[index][0],weights_only=False)\n      video = torch.load(self.videos[index][1],weights_only=False)\n      segmented_video = torch.load(self.videos[index][2],weights_only=False)\n\n      landmarks = np.array(landmarks)\n      segmented_video = np.array(segmented_video)\n      video = np.array(video)\n      \n      if self.agumentation:\n          randomStarting = torch.randint(0, int(len(video)/2), (1,)).item()\n      else:\n          randomStarting = 0\n      \n      indices = torch.linspace(randomStarting, len(video)-1, steps=32).long()\n      \n      segmented_video = segmented_video[indices]\n      landmarks = landmarks[indices]\n      video = video[indices]\n      if self.agumentation:\n          video, segmented_video, agumented_landmarks = self.apply_consistent_augmentation(video,segmented_video, landmarks[:,:-2,:])\n          landmarks[:,:-2,:]= agumented_landmarks\n      cropped_images, mediapipeFeatures = self.mediapipeFeaturesExtractor.extractFeaturesFromVideo(video, landmarks, segmented_video)\n      \n      return mediapipeFeatures, cropped_images, self.labels[index]\n      \n\n  def apply_consistent_augmentation(self,frames,segmented_frames, landmarks):\n    # Generate consistent augmentation parameters for all frames\n    flip = random.random() > 0.5\n    angle = random.uniform(-0.1, 0.1)  # Degrees\n    brightness = random.uniform(0.50, 1.20)\n    contrast = random.uniform(0.8, 1.2)\n    zoom_factor = random.uniform(1, 1.2)\n\n    augmented_frames = []\n    augmented_landmarks = []\n    agumented_segmented_frames =[]\n\n    \n    for frame, segmented_frame, frame_landmarks in zip(frames,segmented_frames, landmarks):\n        \n        # Convert frame to PIL Image\n        frame_pil = Image.fromarray(frame)\n        segmented_frame_pil = Image.fromarray(segmented_frame)\n        width, height = frame_pil.size  # Original size\n\n        # Ensure landmarks are a float32 tensor\n        if not isinstance(frame_landmarks, torch.Tensor):\n            frame_landmarks = torch.tensor(frame_landmarks, dtype=torch.float32)\n        else:\n            frame_landmarks = frame_landmarks.to(dtype=torch.float32)\n\n        # # --- Step 1: Horizontal Flip ---\n        # if flip:\n        #     frame_pil = frame_pil.transpose(Image.FLIP_LEFT_RIGHT)\n        #     frame_landmarks[:, 0] = width - frame_landmarks[:, 0]  # Flip x-coordinates\n        #     segmented_frame_pil = segmented_frame_pil.transpose(Image.FLIP_LEFT_RIGHT)\n\n        # --- Step 2: Rotation ---\n        if angle != 0:\n            # Rotate image (expand=False to maintain original size)\n            frame_pil = frame_pil.rotate(angle, resample=Image.BILINEAR, expand=False)\n            segmented_frame_pil = segmented_frame_pil.rotate(angle, resample=Image.BILINEAR, expand=False)\n            # Rotate landmarks around the image center\n            center = torch.tensor([width / 2, height / 2], dtype=torch.float32)\n            angle_rad = torch.deg2rad(torch.tensor(angle, dtype=torch.float32))\n            cos_theta = torch.cos(angle_rad)\n            sin_theta = torch.sin(angle_rad)\n\n            # Rotation matrix\n            rotation_matrix = torch.tensor([\n                [cos_theta, -sin_theta],\n                [sin_theta, cos_theta]\n            ], dtype=torch.float32)\n\n            # Apply rotation: (landmarks - center) @ R + center\n            frame_landmarks = (frame_landmarks - center) @ rotation_matrix + center\n\n        # --- Step 3: Brightness/Contrast (no landmark change) ---\n        frame_pil = transforms.functional.adjust_brightness(frame_pil, brightness)\n        frame_pil = transforms.functional.adjust_contrast(frame_pil, contrast)\n        \n        segmented_frame_pil = transforms.functional.adjust_brightness(segmented_frame_pil, brightness)\n        segmented_frame_pil = transforms.functional.adjust_contrast(segmented_frame_pil, contrast)\n\n        # --- Step 4: Zoom & Crop ---\n        # Calculate new dimensions and offsets (as floats)\n        new_width = width * zoom_factor\n        new_height = height * zoom_factor\n        left = (new_width - width) / 2  # Float precision\n        top = (new_height - height) / 2\n\n        # Resize image (using integer dimensions)\n        new_width_int = int(round(new_width))\n        new_height_int = int(round(new_height))\n        frame_pil = frame_pil.resize((new_width_int, new_height_int), Image.BILINEAR)\n\n        # Crop to original size (using integer offsets)\n        left_int = int(round(left))\n        top_int = int(round(top))\n        frame_pil = frame_pil.crop((left_int, top_int, left_int + width, top_int + height))\n        segmented_frame_pil = segmented_frame_pil.crop((left_int, top_int, left_int + width, top_int + height))\n\n        # Adjust landmarks (using precise float values)\n        frame_landmarks = (frame_landmarks * zoom_factor) - torch.tensor([left, top], dtype=torch.float32)\n\n        # --- Finalize ---\n        agumented_segmented_frames.append(segmented_frame_pil)\n        augmented_frames.append(frame_pil)\n        augmented_landmarks.append(frame_landmarks)\n    \n    return np.array(augmented_frames).astype(dtype=np.uint16),np.array(agumented_segmented_frames).astype(dtype=np.uint16), np.array(augmented_landmarks)\n\n  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_cell_guid":"52e13902-27c0-4033-a4f5-77a458bfea99","_uuid":"9dd679b9-8d12-480b-9e9f-d787857d961c","execution":{"iopub.status.busy":"2025-07-01T05:43:07.874367Z","iopub.execute_input":"2025-07-01T05:43:07.874554Z","iopub.status.idle":"2025-07-01T05:43:07.926942Z","shell.execute_reply.started":"2025-07-01T05:43:07.874531Z","shell.execute_reply":"2025-07-01T05:43:07.926288Z"},"papermill":{"duration":0.084423,"end_time":"2025-06-28T23:46:40.159321","exception":false,"start_time":"2025-06-28T23:46:40.074898","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"b091934a","cell_type":"code","source":"str(\"magdi\").endswith(\"i\")","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.927889Z","iopub.execute_input":"2025-07-01T05:43:07.928162Z","iopub.status.idle":"2025-07-01T05:43:07.947641Z","shell.execute_reply.started":"2025-07-01T05:43:07.928137Z","shell.execute_reply":"2025-07-01T05:43:07.947108Z"},"papermill":{"duration":0.012381,"end_time":"2025-06-28T23:46:40.178090","exception":false,"start_time":"2025-06-28T23:46:40.165709","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"id":"f79e6205","cell_type":"code","source":"\nCHECKPOINT_PATH = \"/kaggle/input/attention-based-multi-module-fusion/data\"\nBASE_PATH = '/kaggle/input'\nm = [\n    \"wait\",\n    \"bring\",\n    \"friend\",\n    \"good\",\n    \"get_well\",\n    \"get-well\",\n    \"thanks\",\n    \"who\",\n    \"where\",\n    \"why\",\n    \"time\",\n    \"tomorrow\",\n    \"always\",\n    \"apologize\",\n    \"goodbye\",\n    \"single\",\n    \"same\",\n    \"hurry\",\n    \"belt\",\n    \"congratulation\",\n    \"police\",\n    \"single\",\n    \"accident\",\n    \"bed\",\n    \"breakfast\",\n    \"forbidden\",\n    \"sibling\",\n    \"angle\",\n    \"glove\",\n    \"full\"\n        ]\ntester = []\ntrain_video_pathes = []\ntrain_labels = []\nfor gloss in os.listdir(BASE_PATH):\n  if not gloss.__contains__(\"segmented\") :\n      continue\n  glossName = os.listdir(os.path.join(BASE_PATH,gloss))[0]\n  if glossName in m:\n    for video_path in os.listdir(os.path.join(BASE_PATH,gloss,glossName)):\n        files = os.listdir(os.path.join(BASE_PATH,gloss,glossName,video_path))\n        tester.append(video_path)\n        train_video_pathes.append([os.path.join(BASE_PATH,gloss,glossName,video_path,files[0]),os.path.join(BASE_PATH,gloss,glossName,video_path,files[1]),os.path.join(BASE_PATH,gloss,glossName,video_path,files[2])])\n        train_labels.append(glossName.lower())\n\nBASE_PATH = '/kaggle/input'\nfor gloss in os.listdir(BASE_PATH):\n  if not gloss.endswith(\"-gloss\") or not gloss.split(\"-gloss\")[0] in m :  \n      continue\n  glossName = gloss.split(\"-\")[0]   \n  for video_path in os.listdir(os.path.join(BASE_PATH,gloss)):\n    if video_path.endswith(\"json\"):\n        continue\n    files = os.listdir(os.path.join(BASE_PATH,gloss,video_path))\n    tester.append(video_path)\n    train_video_pathes.append([os.path.join(BASE_PATH,gloss,video_path,files[0]),os.path.join(BASE_PATH,gloss,video_path,files[1]),os.path.join(BASE_PATH,gloss,video_path,files[2])])\n    train_labels.append(glossName.lower())\n\ntest_video_pathes = []\ntest_labels = []\nfor t in range(1,6):\n    BASE_PATH = f'/kaggle/input/test-segmented-continue-part{t}-db/gloss'\n    for gloss in os.listdir(BASE_PATH):\n      if gloss in m:\n        for video_path in os.listdir(os.path.join(BASE_PATH,gloss)):\n            files = os.listdir(os.path.join(BASE_PATH,gloss,video_path))\n            test_video_pathes.append([os.path.join(BASE_PATH,gloss,video_path,files[0]),os.path.join(BASE_PATH,gloss,video_path,files[1]),os.path.join(BASE_PATH,gloss,video_path,files[2])])\n            test_labels.append(gloss.lower())\nlen(train_video_pathes)\n\n# BASE_PATH = \"/kaggle/input/test-continue-part2-db/output\"\n# for gloss in os.listdir(BASE_PATH):\n#   if gloss in m:\n#     for video_path in os.listdir(os.path.join(BASE_PATH,gloss)):\n#         files = os.listdir(os.path.join(BASE_PATH,gloss,video_path))\n#         test_video_pathes.append([os.path.join(BASE_PATH,gloss,video_path,files[0]),os.path.join(BASE_PATH,gloss,video_path,files[1])])\n#         test_labels.append(gloss.lower())\nlen(train_video_pathes)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:07.948353Z","iopub.execute_input":"2025-07-01T05:43:07.948569Z","iopub.status.idle":"2025-07-01T05:43:09.966889Z","shell.execute_reply.started":"2025-07-01T05:43:07.948544Z","shell.execute_reply":"2025-07-01T05:43:09.966270Z"},"papermill":{"duration":13.192312,"end_time":"2025-06-28T23:46:53.376089","exception":false,"start_time":"2025-06-28T23:46:40.183777","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"3243"},"metadata":{}}],"execution_count":10},{"id":"a0f486f6","cell_type":"code","source":"len(set(train_labels)),len(set(test_labels))","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:09.967537Z","iopub.execute_input":"2025-07-01T05:43:09.967712Z","iopub.status.idle":"2025-07-01T05:43:09.972617Z","shell.execute_reply.started":"2025-07-01T05:43:09.967698Z","shell.execute_reply":"2025-07-01T05:43:09.972091Z"},"papermill":{"duration":0.011852,"end_time":"2025-06-28T23:46:53.394193","exception":false,"start_time":"2025-06-28T23:46:53.382341","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(26, 26)"},"metadata":{}}],"execution_count":11},{"id":"5c660968","cell_type":"code","source":"print(len(tester),len(set(tester)),len(test_labels))\ntrain_dictanory = {t:0 for t in set(np.sort(train_labels))}\ntest_dictanory = {t:0 for t in set(np.sort(test_labels))}\nfor t in train_labels:\n    train_dictanory[t]+=1\n    \nfor t in test_labels:\n    test_dictanory[t]+=1\n\ntrain_dictanory,test_dictanory","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:09.973307Z","iopub.execute_input":"2025-07-01T05:43:09.973522Z","iopub.status.idle":"2025-07-01T05:43:09.993965Z","shell.execute_reply.started":"2025-07-01T05:43:09.973490Z","shell.execute_reply":"2025-07-01T05:43:09.993262Z"},"papermill":{"duration":0.015737,"end_time":"2025-06-28T23:46:53.415637","exception":false,"start_time":"2025-06-28T23:46:53.399900","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"3243 3243 179\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"({'bed': 127,\n  'single': 126,\n  'thanks': 126,\n  'accident': 125,\n  'get_well': 124,\n  'tomorrow': 117,\n  'hurry': 125,\n  'time': 123,\n  'apologize': 127,\n  'goodbye': 127,\n  'breakfast': 125,\n  'where': 127,\n  'who': 126,\n  'belt': 127,\n  'wait': 125,\n  'glove': 122,\n  'why': 127,\n  'forbidden': 123,\n  'same': 120,\n  'full': 127,\n  'friend': 121,\n  'police': 126,\n  'always': 123,\n  'bring': 124,\n  'sibling': 126,\n  'good': 127},\n {'bed': 8,\n  'single': 7,\n  'thanks': 8,\n  'accident': 5,\n  'get_well': 6,\n  'tomorrow': 7,\n  'hurry': 6,\n  'time': 7,\n  'apologize': 6,\n  'goodbye': 7,\n  'breakfast': 7,\n  'where': 8,\n  'who': 8,\n  'belt': 6,\n  'wait': 6,\n  'glove': 7,\n  'why': 6,\n  'forbidden': 7,\n  'same': 7,\n  'full': 8,\n  'friend': 6,\n  'police': 7,\n  'always': 8,\n  'bring': 8,\n  'sibling': 7,\n  'good': 6})"},"metadata":{}}],"execution_count":12},{"id":"c0a83e1e","cell_type":"code","source":"train_dataset = VideoDataset(\n    train_video_pathes,\n   train_labels,\n  agumentation=True,temp_agumentation=False,getCropedImageToo=True\n    )\ntrain_dataloader = DataLoader(train_dataset,batch_size=2,shuffle=True,\n    num_workers=8,  # Use 4 workers (Adjust based on CPU cores)\n    pin_memory=True, # Speed up CPU → GPU transfer\n    prefetch_factor=5,\n                                       timeout=60\n                              )\ntest_dataset = VideoDataset(\n    test_video_pathes,\n    test_labels,getCropedImageToo=True\n    )\ntest_dataloader = DataLoader(test_dataset,batch_size=2,shuffle=True,\n     num_workers=4,  # Use 4 workers (Adjust based on CPU cores)\n    pin_memory=True, # Speed up CPU → GPU transfer\n    prefetch_factor=5,\n                                      timeout=60\n                             )","metadata":{"_cell_guid":"c4bc09e2-3410-4599-89e1-b1ddb74f6b0b","_uuid":"85ec740d-6454-4d1d-bea0-e93421746585","execution":{"iopub.status.busy":"2025-07-01T05:43:09.994655Z","iopub.execute_input":"2025-07-01T05:43:09.994891Z","iopub.status.idle":"2025-07-01T05:43:10.009550Z","shell.execute_reply.started":"2025-07-01T05:43:09.994872Z","shell.execute_reply":"2025-07-01T05:43:10.008856Z"},"papermill":{"duration":0.029452,"end_time":"2025-06-28T23:46:53.451103","exception":false,"start_time":"2025-06-28T23:46:53.421651","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"id":"d58af7b9","cell_type":"code","source":"# for i in train_dataset[0][0]:\n#     print(i.sum())","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.010375Z","iopub.execute_input":"2025-07-01T05:43:10.010614Z","iopub.status.idle":"2025-07-01T05:43:10.029363Z","shell.execute_reply.started":"2025-07-01T05:43:10.010599Z","shell.execute_reply":"2025-07-01T05:43:10.028814Z"},"papermill":{"duration":0.010195,"end_time":"2025-06-28T23:46:53.467777","exception":false,"start_time":"2025-06-28T23:46:53.457582","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":14},{"id":"9e54df78","cell_type":"code","source":"# torch.load(\"/kaggle/input/who-segmented-continue-part1-db/who/signer0_sample1084/segmented_video.pt\",weights_only=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.030051Z","iopub.execute_input":"2025-07-01T05:43:10.030556Z","iopub.status.idle":"2025-07-01T05:43:10.042642Z","shell.execute_reply.started":"2025-07-01T05:43:10.030538Z","shell.execute_reply":"2025-07-01T05:43:10.042100Z"},"papermill":{"duration":0.010206,"end_time":"2025-06-28T23:46:53.484013","exception":false,"start_time":"2025-06-28T23:46:53.473807","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"id":"27c0e515","cell_type":"code","source":"# for i in test_dataset[0][1]:\n#     plt.imshow(i.permute(1,2,0)/255)\n#     plt.show()\n# for i in test_dataset[0][0]:\n#     plt.imshow(i[19200:19200*2].reshape(80,80,3)/255)\n#     plt.show()\n# train_dataset[0][0].shape\n# train_dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.043370Z","iopub.execute_input":"2025-07-01T05:43:10.043595Z","iopub.status.idle":"2025-07-01T05:43:10.058215Z","shell.execute_reply.started":"2025-07-01T05:43:10.043571Z","shell.execute_reply":"2025-07-01T05:43:10.057634Z"},"papermill":{"duration":0.011433,"end_time":"2025-06-28T23:46:53.501427","exception":false,"start_time":"2025-06-28T23:46:53.489994","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":16},{"id":"0bd21009","cell_type":"code","source":"# mpfeatures, img,label =train_dataset[10]\n# for i in range(32):\n#     plt.imshow(img[i].permute(1,2,0))\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.058833Z","iopub.execute_input":"2025-07-01T05:43:10.059075Z","iopub.status.idle":"2025-07-01T05:43:10.072325Z","shell.execute_reply.started":"2025-07-01T05:43:10.059059Z","shell.execute_reply":"2025-07-01T05:43:10.071639Z"},"papermill":{"duration":0.010326,"end_time":"2025-06-28T23:46:53.517591","exception":false,"start_time":"2025-06-28T23:46:53.507265","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"id":"c6b4e4ad","cell_type":"code","source":"# mpfeatures, img,label =train_dataset[0]\n# for i in range(32):\n#     plt.imshow(mpfeatures[i][:19200].reshape(80,80,3)/255)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.073086Z","iopub.execute_input":"2025-07-01T05:43:10.073331Z","iopub.status.idle":"2025-07-01T05:43:10.087542Z","shell.execute_reply.started":"2025-07-01T05:43:10.073308Z","shell.execute_reply":"2025-07-01T05:43:10.087008Z"},"papermill":{"duration":0.01074,"end_time":"2025-06-28T23:46:53.534424","exception":false,"start_time":"2025-06-28T23:46:53.523684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"68055fb4","cell_type":"code","source":"# next(iter(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.088187Z","iopub.execute_input":"2025-07-01T05:43:10.088460Z","iopub.status.idle":"2025-07-01T05:43:10.103047Z","shell.execute_reply.started":"2025-07-01T05:43:10.088436Z","shell.execute_reply":"2025-07-01T05:43:10.102433Z"},"papermill":{"duration":0.010468,"end_time":"2025-06-28T23:46:53.550845","exception":false,"start_time":"2025-06-28T23:46:53.540377","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"fa0b0cfa","cell_type":"code","source":"\ndef predict(model,test_loader):\n    predictions = []\n    actual = []\n    model.eval()\n    with torch.no_grad():\n        for x,croped_image,y in tqdm(test_loader):    \n            x = x.float().to(device) # batch_size, num_frames, channels, height, width\n            croped_image = croped_image.float().to(device)\n            outputs = model(x,croped_image)\n            predictions.extend(outputs.cpu().detach().numpy() )\n            actual.extend(y.cpu().detach().numpy())\n    return predictions, actual\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self):\n        super(PositionalEmbedding, self).__init__()\n        self.pos_embed = nn.Embedding(32, 2048)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, device=x.device).unsqueeze(0).repeat(x.size(0), 1)  \n        pos_embed = self.pos_embed(pos) \n        return x + pos_embed\n\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, image_dim, pose_dim, project_dim, num_heads=4):\n        super(CrossAttention, self).__init__()\n        self.image_pre_norm = nn.LayerNorm(normalized_shape=image_dim)\n        self.pose_pre_norm = nn.LayerNorm(normalized_shape=pose_dim)\n\n        self.pose_project = nn.Linear(pose_dim, project_dim)\n        self.image_project = nn.Linear(image_dim, project_dim)\n\n        self.image_self_attention= nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n        self.pose_self_Attenstion = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n        \n        self.image_layer_norm = nn.LayerNorm(normalized_shape=project_dim)\n        self.pose_layer_norm = nn.LayerNorm(normalized_shape=project_dim)\n        # self.output_layer_norm = nn.LayerNorm(normalized_shape=int(project_dim*2))\n\n\n        self.cross_attention1 = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n        self.cross_attention2 = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n        \n        \n    def forward(self, image_input, pose_input):\n        image_features = self.image_pre_norm(image_input)\n        pose_features = self.pose_pre_norm(pose_input)\n        \n        pose_projection = self.pose_project(pose_features)\n        image_projection = self.image_project(image_features)\n\n        pose_projection_attention, _ = self.pose_self_Attenstion(query=pose_projection, key=pose_projection, value=pose_projection)\n        image_projection_attention, _ = self.image_self_attention(query=image_projection, key=image_projection, value=image_projection)\n\n        residual_image_connection = image_projection+image_projection_attention\n        residual_pose_connection = pose_projection+pose_projection_attention\n        \n        image = self.image_layer_norm(residual_image_connection)\n        pose = self.pose_layer_norm(residual_pose_connection)\n        \n        attn_output1, _ = self.cross_attention1(query=image, key=pose, value=pose)\n        attn_output2, _ = self.cross_attention2(query=pose, key=image, value=image)\n        \n        # output = self.final_feedforward_layer(torch.cat([ attn_output1, attn_output2 ],dim=2))\n\n        # output = self.output_layer_norm(output)\n\n        return torch.cat([ attn_output1, attn_output2 ],dim=2)  \n\nclass Model(nn.Module):\n    def __init__(self,embedding = 5):\n        super().__init__()\n\n        self.crossAttentionLayer = CrossAttention(image_dim=6320, pose_dim=3102,project_dim=1024, num_heads=4)\n\n        \n        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n        mobilenet_features = mobilenet.features\n\n        t=0\n        for layer in mobilenet.parameters():\n            t+=1\n\n        for i,layer in enumerate(mobilenet.parameters()):\n            layer.requires_grad = False\n            if i== (t-4) or i== (t-5):\n              layer.requires_grad = True\n                \n        self.cropedImageCNN = nn.Sequential(\n                mobilenet_features,\n               \n                nn.Conv2d(1280, 1580, kernel_size=3),\n                nn.BatchNorm2d(1580),\n                nn.SiLU(),\n                nn.AvgPool2d(kernel_size=2),\n            \n                nn.Conv2d(1580, 1580, kernel_size=3),\n                nn.BatchNorm2d(1580),\n                nn.SiLU(),\n                nn.AvgPool2d(kernel_size=2),\n                \n                nn.Flatten()\n            )\n\n       \n        self.leftHandCNN = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 40x40\n        \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 20x20\n        \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 10x10\n        \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 5x5\n            nn.Flatten(),\n            nn.Linear(256 * 5 * 5, 1280),\n            \n        )\n\n        self.rightHandCNN = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 40x40\n        \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 20x20\n        \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 10x10\n        \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.SiLU(),\n            nn.AvgPool2d(2),       # -> 5x5\n            nn.Flatten(),\n            nn.Linear(256 * 5 * 5, 1280),\n            \n        )\n\n        self.leftHandVRegionEmbedding = nn.Sequential(\n            nn.Linear(1,embedding),\n        )\n        self.leftHandHRegionEmbedding = nn.Sequential(\n            nn.Linear(1,embedding),\n        )\n        self.rightHandVRegionEmbedding = nn.Sequential(\n            nn.Linear(1,embedding),\n        )\n        self.rightHandHRegionEmbedding = nn.Sequential(\n            nn.Linear(1,embedding),\n        )\n        self.leftHandDistance = nn.Sequential(\n            nn.Linear(2,embedding),\n        )\n        self.rightHandDistance = nn.Sequential(\n            nn.Linear(2,embedding),\n        )\n        self.landmarksEmbedding = nn.Sequential(\n            nn.Linear(1106 ,512),            \n        ) \n\n        encoder_layer = nn.TransformerEncoderLayer(\n                d_model=2048,\n                nhead=8,\n                dim_feedforward=3072,\n                dropout=0.0,\n                activation='gelu',\n                batch_first=True\n            )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n\n        \n        self.positionalEmbedding = PositionalEmbedding()\n\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 26),\n        )\n        self.landmarksLayerNorm = nn.LayerNorm(normalized_shape=1106)\n        # self.dropout1 = nn.Dropout(0.05)\n        # self.dropout2 = nn.Dropout(0.05)\n        \n    def forward(self,x,croped_image, train= False):\n        batch_size,frames, features = x.size()\n        \n        # print(x.size())\n        \n        # print(x.size())\n            \n        LHI,RHI,LHVR,RHVR,LHHR,RHHR,LHD,RHD,ALL_LANDMARKS =x[:,:,:19200]/255,x[:,:,19200:38400]/255,x[:,:,38400],x[:,:,38401],x[:,:,38402],x[:,:,38403],x[:,:,38404:38406],x[:,:,38406:38408],x[:,:,38408:]\n        LHI = LHI.reshape(batch_size,frames,80,80,3).float()\n        LHI = LHI.reshape(batch_size*frames, 80,80,3)\n        LHI = LHI.permute(0,3,1,2)\n        LHI_output = self.leftHandCNN(LHI)\n        LHI_output = LHI_output.reshape(batch_size,frames,-1) #1280\n        \n        RHI = RHI.reshape(batch_size,frames,80,80,3).float()\n        RHI = RHI.reshape(batch_size*frames, 80,80,3)\n        RHI = RHI.permute(0,3,1,2)\n        # print(RHI.shape)\n        RHI_output = self.rightHandCNN(RHI)\n        RHI_output = RHI_output.reshape(batch_size,frames,-1) #1280\n\n        # print('left hand regoin',LHR.shape)\n        LHVR_output = self.leftHandVRegionEmbedding(LHVR.unsqueeze(-1).float())\n        LHHR_output = self.leftHandHRegionEmbedding(LHHR.unsqueeze(-1).float())\n        RHVR_output = self.rightHandVRegionEmbedding(RHVR.unsqueeze(-1).float())\n        RHHR_output = self.rightHandHRegionEmbedding(RHHR.unsqueeze(-1).float())\n\n        LHD_output = self.leftHandDistance(LHD.float())\n        RHD_output = self.rightHandDistance(RHD.float())\n\n        all_landmarks_output = self.landmarksEmbedding(self.landmarksLayerNorm(ALL_LANDMARKS.float()))\n        \n        batch_size,frames,c,w,h = croped_image.shape\n        croped_images_input = croped_image.reshape(batch_size*frames,c,w,h)\n\n        croped_output = self.cropedImageCNN(croped_images_input/255)#[128, 512, 3, 3]\n\n        croped_output = croped_output.reshape(batch_size,frames,-1)\n        # croped_output = self.dropout1(croped_output)\n        # print(croped_output.shape)\n        flattend_output = torch.cat([LHI_output, RHI_output,LHVR_output,LHHR_output, RHVR_output,RHHR_output,LHD_output, RHD_output, all_landmarks_output],dim=2)\n        # flattend_output = self.dropout2(flattend_output)\n        # flattend_output = self.dropout(flattend_output)\n        combined_output = self.crossAttentionLayer(croped_output,flattend_output)\n        combined_output = self.positionalEmbedding(combined_output)\n        \n        transformer_output = self.transformer(combined_output)\n\n        global_average_pooling = transformer_output.mean(dim=1)\n\n\n        output = self.classifier(global_average_pooling)\n        \n        \n        return output\n\n\n\n{val:key for key,val in train_dataset.label_to_index.items()},{val:key for key,val in test_dataset.label_to_index.items()}","metadata":{"_cell_guid":"26ea7d98-b12c-47b4-86b5-320b9d945d54","_uuid":"6cd7c2db-9dc2-443d-82ff-a83c15035091","collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T05:43:10.103828Z","iopub.execute_input":"2025-07-01T05:43:10.104062Z","iopub.status.idle":"2025-07-01T05:43:10.136687Z","shell.execute_reply.started":"2025-07-01T05:43:10.104048Z","shell.execute_reply":"2025-07-01T05:43:10.136014Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.038554,"end_time":"2025-06-28T23:46:53.595364","exception":false,"start_time":"2025-06-28T23:46:53.556810","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"({0: 'accident',\n  1: 'always',\n  2: 'apologize',\n  3: 'bed',\n  4: 'belt',\n  5: 'breakfast',\n  6: 'bring',\n  7: 'forbidden',\n  8: 'friend',\n  9: 'full',\n  10: 'get_well',\n  11: 'glove',\n  12: 'good',\n  13: 'goodbye',\n  14: 'hurry',\n  15: 'police',\n  16: 'same',\n  17: 'sibling',\n  18: 'single',\n  19: 'thanks',\n  20: 'time',\n  21: 'tomorrow',\n  22: 'wait',\n  23: 'where',\n  24: 'who',\n  25: 'why'},\n {0: 'accident',\n  1: 'always',\n  2: 'apologize',\n  3: 'bed',\n  4: 'belt',\n  5: 'breakfast',\n  6: 'bring',\n  7: 'forbidden',\n  8: 'friend',\n  9: 'full',\n  10: 'get_well',\n  11: 'glove',\n  12: 'good',\n  13: 'goodbye',\n  14: 'hurry',\n  15: 'police',\n  16: 'same',\n  17: 'sibling',\n  18: 'single',\n  19: 'thanks',\n  20: 'time',\n  21: 'tomorrow',\n  22: 'wait',\n  23: 'where',\n  24: 'who',\n  25: 'why'})"},"metadata":{}}],"execution_count":20},{"id":"48c9a443","cell_type":"code","source":"\ndef get_details(actual,predictions):\n    actual_label = np.argmax(np.array(actual),axis=1)\n    predicted_label = np.argmax(predictions, axis =1)\n    accuracy = (actual_label == predicted_label).sum() / len(actual_label)\n    number_of_classes = len(train_dataset.label_to_index)\n    \n    details = {\n        'result_per_label': {i:0 for i in range(number_of_classes)},#how much i classiy for this class\n        'total_per_label':  {i:0 for i in range(number_of_classes)}#how much the total that belong to this class\n    }\n    for i in range(len(actual_label)):\n        result = 1 if actual_label[i] == predicted_label[i] else 0\n        class_index = actual_label[i]\n        details['result_per_label'][class_index]+= result\n        details['total_per_label'][class_index]+= 1\n    index_to_label = {val:key for key,val in train_dataset.label_to_index.items()}\n    deta = {}\n    for i in range(number_of_classes):\n        if details['total_per_label'][i] ==0 or details['result_per_label'][i] ==0 :\n            print('index: ',i,' Gloss: ',index_to_label[i],' accuracy: 0')\n        else:\n            print('index: ',i,' Gloss: ',index_to_label[i],' accuracy: ',(details['result_per_label'][i]/details['total_per_label'][i]))\n            deta [i] = 'index: ' +str(i)+ ' Gloss: '+ str(index_to_label[i])+' accuracy: '+ str(details['result_per_label'][i]/details['total_per_label'][i])\n    return accuracy, deta\n    \ndef train(model,classification_loss_fn,epoch,train_loader,test_loader,scheduler,test_accuracy):\n    max_seconds = 10.5*60*60  # 10 hours and 30 minutes\n    start_time = time.time()\n    if os.path.exists(os.path.join(CHECKPOINT_PATH,'checkpoint.pth')):\n        checkpointsaver = torch.load(os.path.join(CHECKPOINT_PATH,'checkpoint.pth'), map_location='cpu')\n        with open(os.path.join(CHECKPOINT_PATH,'data.json'), 'r') as file:\n            metadatasaver = json.load(file)\n    else:\n        checkpointsaver = {}\n        metadatasaver = {}\n\n    total_loss = []\n    glosses_details =[]\n    current_best_accuracy = 0\n    for ep in range(epoch,epoch+1000):\n        model.train()\n        total_loss_per_epoch = 0\n        accuarcy = 0\n        total_samples= 0\n        for i, (x, croped_images, y) in enumerate(train_loader):\n            elapsed = time.time() - start_time\n            if elapsed > max_seconds:\n                print(f\"\\n⏱️ Training loop exceeded 9 hours 30 minutes. Breaking at batch {i}.\")\n                return None\n\n            optimizer.zero_grad()\n        \n            x = x.float().to(device)\n            croped_images = croped_images.float().to(device)\n        \n            if y.ndim > 1:  # one-hot -> indices\n                y = torch.argmax(y, dim=1)\n        \n            y = y.long().to(device)\n        \n            classification_output = model(x, croped_images)  # shape: [B, 2]\n        \n            # Compute loss\n            classification_loss = classification_loss_fn(classification_output, y)\n            loss = classification_loss\n            total_loss_per_epoch += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        \n            # Compute accuracy\n            predictions = torch.argmax(classification_output, dim=1)\n            accuarcy += (predictions == y).sum().item()\n            total_samples += y.size(0)\n        \n            print(f\"\\rEpoch {ep+1}, loss: {(total_loss_per_epoch/(i+1)):.8f}, [{i}/{len(train_loader)}]\", end=\"\")\n\n        print(f\"\\rEpoch {ep+1}, loss: {(total_loss_per_epoch/len(train_loader)):.8f}\")\n        print()\n        print(\"train accuracy is: \",(accuarcy/total_samples))\n        avg_loss = total_loss_per_epoch/len(train_loader)\n        total_loss.append(avg_loss)\n        actual,predictions = predict(model,test_loader)\n        acc, deta = get_details(actual,predictions)\n        \n        \n        \n\n        \n        checkpointsaver['model_state_dict'] =  model.state_dict()\n        checkpointsaver['optimizer_state_dict'] =  optimizer.state_dict()\n        checkpointsaver['scheduler_state_dict'] =  scheduler.state_dict()\n        checkpointsaver[\"epoch\"] =  ep+1\n\n        \n         \n        metadatasaver[ \"epoch\"]= ep\n        metadatasaver[ \"total train accuracy\"]=(accuarcy/total_samples)\n        metadatasaver[ \"total test accuracy\"]= acc\n        metadatasaver[\"glosses test accuracy\"]= deta\n        \n\n        \n        if acc >= test_accuracy:\n            test_accuracy =acc\n            metadatasaver['best test accuracy'] = test_accuracy\n            metadatasaver['best test accuracy details'] = deta\n            checkpointsaver['best_model_state_dict']=model.state_dict()\n            \n        print('total accuracy: ',acc)\n        with open(\"data.json\", \"w\") as f:\n            json.dump(metadatasaver, f,indent=4)\n        print(\"best test accuracy: \",test_accuracy)\n\n\n        torch.save(checkpointsaver, \"checkpoint.pth\")\n\n        scheduler.step(avg_loss)\n        print(\"\\n///////////current learning rate: \",optimizer.param_groups[0]['lr'],\" /////////////\\n\")\n\n    return total_loss\n    ","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:10.139467Z","iopub.execute_input":"2025-07-01T05:43:10.139639Z","iopub.status.idle":"2025-07-01T05:43:10.159040Z","shell.execute_reply.started":"2025-07-01T05:43:10.139627Z","shell.execute_reply":"2025-07-01T05:43:10.158350Z"},"papermill":{"duration":0.020637,"end_time":"2025-06-28T23:46:53.622233","exception":false,"start_time":"2025-06-28T23:46:53.601596","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":21},{"id":"39afefee-9cee-4c76-8ff3-ce9500050436","cell_type":"code","source":"os.listdir(CHECKPOINT_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T05:43:10.159875Z","iopub.execute_input":"2025-07-01T05:43:10.160154Z","iopub.status.idle":"2025-07-01T05:43:10.183160Z","shell.execute_reply.started":"2025-07-01T05:43:10.160131Z","shell.execute_reply":"2025-07-01T05:43:10.182502Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['data.json', 'checkpoint.pth']"},"metadata":{}}],"execution_count":22},{"id":"2cf211bf","cell_type":"code","source":"if not os.path.exists(os.path.join(CHECKPOINT_PATH,'checkpoint.pth')):\n    model = Model().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-05)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='min',         \n        factor=0.5,         \n        patience=3,         \n        verbose=True        \n    )\n    \n    start_epoch= 0 \n    test_accuracy = 0\nelse:\n    import torch\n    import gc\n    from torch import nn\n    \n    # 1. Force clean GPU memory\n    def clear_gpu_memory():\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()  # Reset memory tracking\n    \n    clear_gpu_memory()\n    \n    # 2. Load checkpoint differently\n    def load_checkpoint_safely(checkpoint_path, device):\n        # Load to CPU first\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        \n        # Create model on CPU\n        model = Model()  # Don't move to GPU yet\n        with open(os.path.join(CHECKPOINT_PATH,'data.json'), 'r') as file:\n            data = json.load(file)\n            \n        if 'best test accuracy' in data:\n            test_accuracy = data[\"best test accuracy\"]\n        else:\n            test_accuracy = 0\n         # Load state dict in eval mode (uses less memory)\n        model.eval()\n        model.load_state_dict(checkpoint['best_model_state_dict'])\n        \n        # Now move to GPU\n        model = model.to(device)\n        \n        # Create optimizer after model is on GPU\n        optimizer = torch.optim.AdamW(model.parameters(), lr=8e-05)\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        return {\n            'model': model,\n            'optimizer': optimizer,\n            'epoch': checkpoint['epoch'],\n            'scheduler_state': checkpoint.get('scheduler_state_dict', None)\n        },test_accuracy\n    \n\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    # Load checkpoint\n    loaded, test_accuracy = load_checkpoint_safely(os.path.join(CHECKPOINT_PATH,'checkpoint.pth'), device)\n    \n    model = loaded['model']\n    optimizer = loaded['optimizer']\n    start_epoch = loaded['epoch']\n    \n    # Recreate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='min',\n        factor=0.5,\n        patience=3,\n        verbose=True\n    )\n    \n    if loaded['scheduler_state'] is not None:\n        scheduler.load_state_dict(loaded['scheduler_state'])\n                \nclassification_loss = nn.CrossEntropyLoss ()\n\n","metadata":{"_cell_guid":"2b7098eb-b328-4522-92e2-125c3eee33fc","_uuid":"b79d0bd2-3280-426b-a2e8-5f858aa298ba","execution":{"iopub.status.busy":"2025-07-01T05:43:10.183764Z","iopub.execute_input":"2025-07-01T05:43:10.183972Z","iopub.status.idle":"2025-07-01T05:43:23.929335Z","shell.execute_reply.started":"2025-07-01T05:43:10.183958Z","shell.execute_reply":"2025-07-01T05:43:23.928468Z"},"papermill":{"duration":12.072946,"end_time":"2025-06-28T23:47:05.712964","exception":false,"start_time":"2025-06-28T23:46:53.640018","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"id":"e3cbcad6","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.011471,"end_time":"2025-06-28T23:47:05.731377","exception":false,"start_time":"2025-06-28T23:47:05.719906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"38d1817e","cell_type":"code","source":"print(\"started epoch: \", start_epoch)\nprint(\"test accuracy: \",test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:23.930327Z","iopub.execute_input":"2025-07-01T05:43:23.930595Z","iopub.status.idle":"2025-07-01T05:43:23.934749Z","shell.execute_reply.started":"2025-07-01T05:43:23.930577Z","shell.execute_reply":"2025-07-01T05:43:23.934163Z"},"papermill":{"duration":0.011884,"end_time":"2025-06-28T23:47:05.749582","exception":false,"start_time":"2025-06-28T23:47:05.737698","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"started epoch:  82\ntest accuracy:  0.8938547486033519\n","output_type":"stream"}],"execution_count":24},{"id":"1fcda305","cell_type":"code","source":"# print(\"model classifier: \", model.classifier)","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:23.935475Z","iopub.execute_input":"2025-07-01T05:43:23.935692Z","iopub.status.idle":"2025-07-01T05:43:23.955414Z","shell.execute_reply.started":"2025-07-01T05:43:23.935677Z","shell.execute_reply":"2025-07-01T05:43:23.954862Z"},"papermill":{"duration":0.01116,"end_time":"2025-06-28T23:47:05.767207","exception":false,"start_time":"2025-06-28T23:47:05.756047","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":25},{"id":"af878eb1","cell_type":"code","source":"history = train(model,classification_loss ,start_epoch,train_dataloader,test_dataloader,scheduler,test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2025-07-01T05:43:23.956199Z","iopub.execute_input":"2025-07-01T05:43:23.956451Z"},"papermill":{"duration":37802.218421,"end_time":"2025-06-29T10:17:07.991917","exception":false,"start_time":"2025-06-28T23:47:05.773496","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 83, loss: 0.15415485, [1621/1622]\n\ntrain accuracy is:  0.9759481961147086\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 90/90 [01:02<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"index:  0  Gloss:  accident  accuracy:  1.0\nindex:  1  Gloss:  always  accuracy:  0.6363636363636364\nindex:  2  Gloss:  apologize  accuracy:  1.0\nindex:  3  Gloss:  bed  accuracy:  0.6153846153846154\nindex:  4  Gloss:  belt  accuracy:  1.0\nindex:  5  Gloss:  breakfast  accuracy:  1.0\nindex:  6  Gloss:  bring  accuracy:  0.7777777777777778\nindex:  7  Gloss:  forbidden  accuracy:  1.0\nindex:  8  Gloss:  friend  accuracy:  1.0\nindex:  9  Gloss:  full  accuracy:  1.0\nindex:  10  Gloss:  get_well  accuracy:  0.8333333333333334\nindex:  11  Gloss:  glove  accuracy:  0.8333333333333334\nindex:  12  Gloss:  good  accuracy:  1.0\nindex:  13  Gloss:  goodbye  accuracy:  0.8571428571428571\nindex:  14  Gloss:  hurry  accuracy:  0.8\nindex:  15  Gloss:  police  accuracy:  0.625\nindex:  16  Gloss:  same  accuracy:  0.625\nindex:  17  Gloss:  sibling  accuracy:  1.0\nindex:  18  Gloss:  single  accuracy:  0.875\nindex:  19  Gloss:  thanks  accuracy:  0.8\nindex:  20  Gloss:  time  accuracy:  1.0\nindex:  21  Gloss:  tomorrow  accuracy:  1.0\nindex:  22  Gloss:  wait  accuracy:  1.0\nindex:  23  Gloss:  where  accuracy:  1.0\nindex:  24  Gloss:  who  accuracy:  0.8888888888888888\nindex:  25  Gloss:  why  accuracy:  1.0\ntotal accuracy:  0.8603351955307262\nbest test accuracy:  0.8938547486033519\n\n///////////current learning rate:  1e-05  /////////////\n\nEpoch 84, loss: 0.08581140, [1621/1622]\n\ntrain accuracy is:  0.9842738205365402\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 90/90 [01:05<00:00,  1.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"index:  0  Gloss:  accident  accuracy:  1.0\nindex:  1  Gloss:  always  accuracy:  0.7272727272727273\nindex:  2  Gloss:  apologize  accuracy:  1.0\nindex:  3  Gloss:  bed  accuracy:  1.0\nindex:  4  Gloss:  belt  accuracy:  1.0\nindex:  5  Gloss:  breakfast  accuracy:  0.7777777777777778\nindex:  6  Gloss:  bring  accuracy:  0.7272727272727273\nindex:  7  Gloss:  forbidden  accuracy:  0.875\nindex:  8  Gloss:  friend  accuracy:  1.0\nindex:  9  Gloss:  full  accuracy:  0.75\nindex:  10  Gloss:  get_well  accuracy:  1.0\nindex:  11  Gloss:  glove  accuracy:  0.8333333333333334\nindex:  12  Gloss:  good  accuracy:  1.0\nindex:  13  Gloss:  goodbye  accuracy:  1.0\nindex:  14  Gloss:  hurry  accuracy:  0.75\nindex:  15  Gloss:  police  accuracy:  0.6363636363636364\nindex:  16  Gloss:  same  accuracy:  1.0\nindex:  17  Gloss:  sibling  accuracy:  0.7777777777777778\nindex:  18  Gloss:  single  accuracy:  0.7777777777777778\nindex:  19  Gloss:  thanks  accuracy:  0.8888888888888888\nindex:  20  Gloss:  time  accuracy:  1.0\nindex:  21  Gloss:  tomorrow  accuracy:  1.0\nindex:  22  Gloss:  wait  accuracy:  1.0\nindex:  23  Gloss:  where  accuracy:  1.0\nindex:  24  Gloss:  who  accuracy:  0.7777777777777778\nindex:  25  Gloss:  why  accuracy:  1.0\ntotal accuracy:  0.8659217877094972\nbest test accuracy:  0.8938547486033519\n\n///////////current learning rate:  1e-05  /////////////\n\nEpoch 85, loss: 0.01805708, [207/1622]","output_type":"stream"}],"execution_count":null},{"id":"55e220a4","cell_type":"code","source":" ","metadata":{"papermill":{"duration":2.822051,"end_time":"2025-06-29T10:17:13.919665","exception":false,"start_time":"2025-06-29T10:17:11.097614","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}