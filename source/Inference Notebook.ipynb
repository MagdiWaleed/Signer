{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-05T08:34:27.645483Z",
     "iopub.status.busy": "2025-07-05T08:34:27.645028Z",
     "iopub.status.idle": "2025-07-05T08:34:45.220012Z",
     "shell.execute_reply": "2025-07-05T08:34:45.219147Z",
     "shell.execute_reply.started": "2025-07-05T08:34:27.645439Z"
    },
    "id": "qkbGpVA2LKA8",
    "outputId": "f0f02ae1-856e-4df7-823e-7b877ffca923",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2.4.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mediapipe) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mediapipe) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->mediapipe) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->mediapipe) (2024.2.0)\n",
      "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: protobuf, sounddevice, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mediapipe-0.10.21 protobuf-4.25.8 sounddevice-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:34:45.222253Z",
     "iopub.status.busy": "2025-07-05T08:34:45.221793Z",
     "iopub.status.idle": "2025-07-05T08:35:04.078442Z",
     "shell.execute_reply": "2025-07-05T08:35:04.077853Z",
     "shell.execute_reply.started": "2025-07-05T08:34:45.222227Z"
    },
    "id": "Ulf_wIWnLKA-",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 08:34:47.106643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751704487.321714      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751704487.383690      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "import torchvision.transforms.functional as TransformerF\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import shutil\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.079587Z",
     "iopub.status.busy": "2025-07-05T08:35:04.079132Z",
     "iopub.status.idle": "2025-07-05T08:35:04.503806Z",
     "shell.execute_reply": "2025-07-05T08:35:04.503069Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.079567Z"
    },
    "id": "LMDMgMm9apJT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = kagglehub.model_download(\"v7wedv/91-accuracy-multi-module-fusion-model/pyTorch/default\")\n",
    "model_path = os.path.join(model_path,\"checkpoint.pth\")\n",
    "boundaryModelPath = kagglehub.dataset_download(\"ggez1244/boundary-model-weights\")\n",
    "boundaryModelPath = os.path.join(boundaryModelPath,\"boundary_model_weights (1).pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.506016Z",
     "iopub.status.busy": "2025-07-05T08:35:04.505670Z",
     "iopub.status.idle": "2025-07-05T08:35:04.528933Z",
     "shell.execute_reply": "2025-07-05T08:35:04.528196Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.505990Z"
    },
    "id": "gmoWzCWSLKA_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pos_embed = nn.Embedding(32, 2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, device=x.device).unsqueeze(0).repeat(x.size(0), 1)\n",
    "        pos_embed = self.pos_embed(pos)\n",
    "        return x + pos_embed\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, image_dim, pose_dim, project_dim, num_heads=4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.image_pre_norm = nn.LayerNorm(normalized_shape=image_dim)\n",
    "        self.pose_pre_norm = nn.LayerNorm(normalized_shape=pose_dim)\n",
    "\n",
    "        self.pose_project = nn.Linear(pose_dim, project_dim)\n",
    "        self.image_project = nn.Linear(image_dim, project_dim)\n",
    "\n",
    "        self.image_self_attention= nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.pose_self_Attenstion = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.image_layer_norm = nn.LayerNorm(normalized_shape=project_dim)\n",
    "        self.pose_layer_norm = nn.LayerNorm(normalized_shape=project_dim)\n",
    "        # self.output_layer_norm = nn.LayerNorm(normalized_shape=int(project_dim*2))\n",
    "\n",
    "\n",
    "        self.cross_attention1 = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.cross_attention2 = nn.MultiheadAttention(embed_dim=project_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, image_input, pose_input):\n",
    "        image_features = self.image_pre_norm(image_input)\n",
    "        pose_features = self.pose_pre_norm(pose_input)\n",
    "\n",
    "        pose_projection = self.pose_project(pose_features)\n",
    "        image_projection = self.image_project(image_features)\n",
    "\n",
    "        pose_projection_attention, _ = self.pose_self_Attenstion(query=pose_projection, key=pose_projection, value=pose_projection)\n",
    "        image_projection_attention, _ = self.image_self_attention(query=image_projection, key=image_projection, value=image_projection)\n",
    "\n",
    "        residual_image_connection = image_projection+image_projection_attention\n",
    "        residual_pose_connection = pose_projection+pose_projection_attention\n",
    "\n",
    "        image = self.image_layer_norm(residual_image_connection)\n",
    "        pose = self.pose_layer_norm(residual_pose_connection)\n",
    "\n",
    "        attn_output1, _ = self.cross_attention1(query=image, key=pose, value=pose)\n",
    "        attn_output2, _ = self.cross_attention2(query=pose, key=image, value=image)\n",
    "\n",
    "        # output = self.final_feedforward_layer(torch.cat([ attn_output1, attn_output2 ],dim=2))\n",
    "\n",
    "        # output = self.output_layer_norm(output)\n",
    "\n",
    "        return torch.cat([ attn_output1, attn_output2 ],dim=2)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,embedding = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.crossAttentionLayer = CrossAttention(image_dim=6320, pose_dim=3102,project_dim=1024, num_heads=4)\n",
    "\n",
    "\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        mobilenet_features = mobilenet.features\n",
    "\n",
    "        t=0\n",
    "        for layer in mobilenet.parameters():\n",
    "            t+=1\n",
    "\n",
    "        for i,layer in enumerate(mobilenet.parameters()):\n",
    "            layer.requires_grad = False\n",
    "            if i== (t-4) or i== (t-5):\n",
    "              layer.requires_grad = True\n",
    "\n",
    "        self.cropedImageCNN = nn.Sequential(\n",
    "                mobilenet_features,\n",
    "\n",
    "                nn.Conv2d(1280, 1580, kernel_size=3),\n",
    "                nn.BatchNorm2d(1580),\n",
    "                nn.SiLU(),\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "                nn.Conv2d(1580, 1580, kernel_size=3),\n",
    "                nn.BatchNorm2d(1580),\n",
    "                nn.SiLU(),\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "                nn.Flatten()\n",
    "            )\n",
    "\n",
    "\n",
    "        self.leftHandCNN = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 40x40\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 20x20\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 10x10\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 5x5\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 5 * 5, 1280),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.rightHandCNN = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 40x40\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 20x20\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 10x10\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2),       # -> 5x5\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 5 * 5, 1280),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.leftHandVRegionEmbedding = nn.Sequential(\n",
    "            nn.Linear(1,embedding),\n",
    "        )\n",
    "        self.leftHandHRegionEmbedding = nn.Sequential(\n",
    "            nn.Linear(1,embedding),\n",
    "        )\n",
    "        self.rightHandVRegionEmbedding = nn.Sequential(\n",
    "            nn.Linear(1,embedding),\n",
    "        )\n",
    "        self.rightHandHRegionEmbedding = nn.Sequential(\n",
    "            nn.Linear(1,embedding),\n",
    "        )\n",
    "        self.leftHandDistance = nn.Sequential(\n",
    "            nn.Linear(2,embedding),\n",
    "        )\n",
    "        self.rightHandDistance = nn.Sequential(\n",
    "            nn.Linear(2,embedding),\n",
    "        )\n",
    "        self.landmarksEmbedding = nn.Sequential(\n",
    "            nn.Linear(1106 ,512),\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=2048,\n",
    "                nhead=8,\n",
    "                dim_feedforward=3072,\n",
    "                dropout=0.0,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "\n",
    "        self.positionalEmbedding = PositionalEmbedding()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 26),\n",
    "        )\n",
    "        self.landmarksLayerNorm = nn.LayerNorm(normalized_shape=1106)\n",
    "        # self.dropout1 = nn.Dropout(0.05)\n",
    "        # self.dropout2 = nn.Dropout(0.05)\n",
    "\n",
    "    def forward(self,x,croped_image, train= False):\n",
    "        batch_size,frames, features = x.size()\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        LHI,RHI,LHVR,RHVR,LHHR,RHHR,LHD,RHD,ALL_LANDMARKS =x[:,:,:19200]/255,x[:,:,19200:38400]/255,x[:,:,38400],x[:,:,38401],x[:,:,38402],x[:,:,38403],x[:,:,38404:38406],x[:,:,38406:38408],x[:,:,38408:]\n",
    "        LHI = LHI.reshape(batch_size,frames,80,80,3).float()\n",
    "        LHI = LHI.reshape(batch_size*frames, 80,80,3)\n",
    "        LHI = LHI.permute(0,3,1,2)\n",
    "        LHI_output = self.leftHandCNN(LHI)\n",
    "        LHI_output = LHI_output.reshape(batch_size,frames,-1) #1280\n",
    "\n",
    "        RHI = RHI.reshape(batch_size,frames,80,80,3).float()\n",
    "        RHI = RHI.reshape(batch_size*frames, 80,80,3)\n",
    "        RHI = RHI.permute(0,3,1,2)\n",
    "        # print(RHI.shape)\n",
    "        RHI_output = self.rightHandCNN(RHI)\n",
    "        RHI_output = RHI_output.reshape(batch_size,frames,-1) #1280\n",
    "\n",
    "        # print('left hand regoin',LHR.shape)\n",
    "        LHVR_output = self.leftHandVRegionEmbedding(LHVR.unsqueeze(-1).float())\n",
    "        LHHR_output = self.leftHandHRegionEmbedding(LHHR.unsqueeze(-1).float())\n",
    "        RHVR_output = self.rightHandVRegionEmbedding(RHVR.unsqueeze(-1).float())\n",
    "        RHHR_output = self.rightHandHRegionEmbedding(RHHR.unsqueeze(-1).float())\n",
    "\n",
    "        LHD_output = self.leftHandDistance(LHD.float())\n",
    "        RHD_output = self.rightHandDistance(RHD.float())\n",
    "\n",
    "        all_landmarks_output = self.landmarksEmbedding(self.landmarksLayerNorm(ALL_LANDMARKS.float()))\n",
    "\n",
    "        batch_size,frames,c,w,h = croped_image.shape\n",
    "        croped_images_input = croped_image.reshape(batch_size*frames,c,w,h)\n",
    "\n",
    "        croped_output = self.cropedImageCNN(croped_images_input/255)#[128, 512, 3, 3]\n",
    "\n",
    "        croped_output = croped_output.reshape(batch_size,frames,-1)\n",
    "        # croped_output = self.dropout1(croped_output)\n",
    "        # print(croped_output.shape)\n",
    "        flattend_output = torch.cat([LHI_output, RHI_output,LHVR_output,LHHR_output, RHVR_output,RHHR_output,LHD_output, RHD_output, all_landmarks_output],dim=2)\n",
    "        # flattend_output = self.dropout2(flattend_output)\n",
    "        # flattend_output = self.dropout(flattend_output)\n",
    "        combined_output = self.crossAttentionLayer(croped_output,flattend_output)\n",
    "        combined_output = self.positionalEmbedding(combined_output)\n",
    "\n",
    "        transformer_output = self.transformer(combined_output)\n",
    "\n",
    "        global_average_pooling = transformer_output.mean(dim=1)\n",
    "\n",
    "\n",
    "        output = self.classifier(global_average_pooling)\n",
    "\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.529780Z",
     "iopub.status.busy": "2025-07-05T08:35:04.529592Z",
     "iopub.status.idle": "2025-07-05T08:35:04.552732Z",
     "shell.execute_reply": "2025-07-05T08:35:04.552026Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.529765Z"
    },
    "id": "M5hUYa-6LKBA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BoundaryDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.src_projection = nn.Sequential(\n",
    "            nn.LayerNorm(33*2),\n",
    "            nn.Linear(33*2,128),\n",
    "            nn.Linear(128,256)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=256,\n",
    "                nhead=4,\n",
    "                dim_feedforward=512,\n",
    "                dropout=0.1,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "\n",
    "        self.positionalEmbedding = PositionalEncoding(256)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128,2)\n",
    "        )\n",
    "\n",
    "    def forward(self,landmarks):\n",
    "        batch_size,seq_len, _,_ = landmarks.size()\n",
    "        landmarks = landmarks.reshape(batch_size, seq_len,-1)\n",
    "        landmarks = self.src_projection(landmarks)\n",
    "\n",
    "        landmarks = self.positionalEmbedding(landmarks)\n",
    "\n",
    "        transformer_out = self.transformer(landmarks)\n",
    "        logits = self.output_layer(transformer_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.553897Z",
     "iopub.status.busy": "2025-07-05T08:35:04.553639Z",
     "iopub.status.idle": "2025-07-05T08:35:04.577857Z",
     "shell.execute_reply": "2025-07-05T08:35:04.577141Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.553875Z"
    },
    "id": "qVLTUoSKLKBA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Extractor():\n",
    "  def __init__(self,debuging,target_size):\n",
    "    self.BG_COLOR = (0, 0, 0)\n",
    "    self.debuging = debuging\n",
    "    self.pose = mp_pose.Pose(\n",
    "        static_image_mode=True,\n",
    "        model_complexity=0,\n",
    "        enable_segmentation=True,\n",
    "        min_detection_confidence=0.1\n",
    "        )\n",
    "\n",
    "    self.hands = mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.1\n",
    "        )\n",
    "\n",
    "    self.face_mesh = mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.1\n",
    "       )\n",
    "\n",
    "\n",
    "  def extractLandmarks(self, video):\n",
    "    if len(video.shape)==4:\n",
    "        landmarks= []\n",
    "        segmented_images = []\n",
    "        images = []\n",
    "        for image in tqdm(video):\n",
    "          info = {}\n",
    "          pose_landmarks = self.extractPose(image)\n",
    "          hands_landmarks = self.extractHands(image)\n",
    "          face_landmarks = self.extractFace(image)\n",
    "\n",
    "          if not pose_landmarks[\"pose_landmark\"]:\n",
    "            info[\"pose_landmarks\"] = None\n",
    "            info[\"body_segmentation\"] = pose_landmarks[\"body_segmentation\"]\n",
    "          else:\n",
    "            info[\"pose_landmarks\"] = pose_landmarks[\"pose_landmark\"]\n",
    "            info[\"body_segmentation\"] = pose_landmarks[\"body_segmentation\"]\n",
    "\n",
    "          if not hands_landmarks:\n",
    "            info[\"hands_landmarks\"] = {\n",
    "                \"right_hand\":None,\n",
    "                \"left_hand\":None,\n",
    "                \"landmarks\":None\n",
    "            }\n",
    "          else:\n",
    "            info[\"hands_landmarks\"] = hands_landmarks\n",
    "\n",
    "          if not face_landmarks:\n",
    "            info[\"face_landmarks\"] = None\n",
    "          else:\n",
    "            info[\"face_landmarks\"] = face_landmarks\n",
    "\n",
    "          checker = self.check(info)\n",
    "          x_y_landmarks = self.get_x_y_landmarks_scaled_up(info,image)\n",
    "\n",
    "          right_hand = 0\n",
    "          left_hand = 0\n",
    "          face = 0\n",
    "          pose = 0\n",
    "          if checker[\"right_hand_landmarks\"]:\n",
    "            right_hand = 1\n",
    "          if checker[\"left_hand_landmarks\"]:\n",
    "            left_hand = 1\n",
    "          if checker[\"pose_landmarks\"]:\n",
    "              pose = 1\n",
    "          if checker[\"face_landmarks\"]:\n",
    "              face =1\n",
    "          x_y_landmarks.append([right_hand,pose])\n",
    "          x_y_landmarks.append([face,left_hand])\n",
    "          landmarks.append(x_y_landmarks)\n",
    "          segmented_images.append(self.getSegmentedImage(info,image))\n",
    "          images.append(image)\n",
    "          if self.debuging:\n",
    "              print(checker)\n",
    "              self.showLandmarks(info,image)\n",
    "        return landmarks, segmented_images, images\n",
    "    else:\n",
    "          image = video\n",
    "          info = {}\n",
    "          pose_landmarks = self.extractPose(image)\n",
    "          hands_landmarks = self.extractHands(image)\n",
    "          face_landmarks = self.extractFace(image)\n",
    "\n",
    "          if not pose_landmarks[\"pose_landmark\"]:\n",
    "            info[\"pose_landmarks\"] = None\n",
    "            info[\"body_segmentation\"] = pose_landmarks[\"body_segmentation\"]\n",
    "          else:\n",
    "            info[\"pose_landmarks\"] = pose_landmarks[\"pose_landmark\"]\n",
    "            info[\"body_segmentation\"] = pose_landmarks[\"body_segmentation\"]\n",
    "\n",
    "          if not hands_landmarks:\n",
    "            info[\"hands_landmarks\"] = {\n",
    "                \"right_hand\":None,\n",
    "                \"left_hand\":None,\n",
    "                \"landmarks\":None\n",
    "            }\n",
    "          else:\n",
    "            info[\"hands_landmarks\"] = hands_landmarks\n",
    "\n",
    "          if not face_landmarks:\n",
    "            info[\"face_landmarks\"] = None\n",
    "          else:\n",
    "            info[\"face_landmarks\"] = face_landmarks\n",
    "\n",
    "          checker = self.check(info)\n",
    "          return checker\n",
    "\n",
    "\n",
    "  def get_x_y_landmarks_scaled_up(self,info,image):\n",
    "    landmarks = self.get_x_y_landmarks(info,image)\n",
    "    shape = image.shape\n",
    "    landmarks = [[t[0]*shape[1],t[1]*shape[0]] for t in landmarks]\n",
    "    return landmarks\n",
    "  def get_x_y_landmarks(self,info, image):\n",
    "\n",
    "    x_y_landmarks = []\n",
    "    if info[\"hands_landmarks\"][\"right_hand\"] != None:\n",
    "      for landmark in info[\"hands_landmarks\"][\"right_hand\"].landmark:\n",
    "        x_y_landmarks.append([landmark.x,landmark.y])\n",
    "    else:\n",
    "      x_y_landmarks.extend(list(np.zeros((21,2))))\n",
    "\n",
    "\n",
    "    if info[\"pose_landmarks\"] != None:\n",
    "      for landmark in info[\"pose_landmarks\"].landmark:\n",
    "        x_y_landmarks.append([landmark.x,landmark.y])\n",
    "    else:\n",
    "      x_y_landmarks.extend(list(np.zeros((33,2))))\n",
    "\n",
    "\n",
    "    if info[\"face_landmarks\"] != None:\n",
    "      for landmark in info[\"face_landmarks\"].landmark:\n",
    "        x_y_landmarks.append([landmark.x,landmark.y])\n",
    "    else:\n",
    "      x_y_landmarks.extend(list(np.zeros((478,2))))\n",
    "\n",
    "\n",
    "    if info[\"hands_landmarks\"][\"left_hand\"] != None:\n",
    "      for landmark in info[\"hands_landmarks\"][\"left_hand\"].landmark:\n",
    "        x_y_landmarks.append([landmark.x,landmark.y])\n",
    "    else:\n",
    "      x_y_landmarks.extend(list(np.zeros((21,2))))\n",
    "\n",
    "    return x_y_landmarks\n",
    "\n",
    "  def check(self,info):\n",
    "    return {\n",
    "        \"pose_landmarks\":info[\"pose_landmarks\"] !=None,\n",
    "        \"right_hand_landmarks\":info[\"hands_landmarks\"][\"right_hand\"] !=None,\n",
    "        \"left_hand_landmarks\":info[\"hands_landmarks\"][\"left_hand\"] !=None,\n",
    "        \"face_landmarks\":info[\"face_landmarks\"]!=None\n",
    "    }\n",
    "  def extractPose(self,image):\n",
    "      results = self.pose.process(image)\n",
    "      if not results.pose_landmarks:\n",
    "        return {\n",
    "            \"pose_landmark\":None,\n",
    "            \"body_segmentation\": results.segmentation_mask\n",
    "            }\n",
    "      return {\n",
    "            \"pose_landmark\":results.pose_landmarks,\n",
    "            \"body_segmentation\": results.segmentation_mask\n",
    "            }\n",
    "\n",
    "\n",
    "  def extractHands(self,image):\n",
    "      results = self.hands.process(image)\n",
    "\n",
    "      if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "      data = {\n",
    "          \"right_hand\":None,\n",
    "          \"left_hand\":None,\n",
    "          \"landmarks\":results.multi_hand_landmarks\n",
    "          }\n",
    "      for handedness, landmarks in zip(results.multi_handedness, results.multi_hand_landmarks):\n",
    "          label = handedness.classification[0].label\n",
    "          if label == \"Right\":\n",
    "              data[\"right_hand\"] = landmarks\n",
    "          elif label == \"Left\":\n",
    "              data[\"left_hand\"] = landmarks\n",
    "      return data\n",
    "\n",
    "\n",
    "  def extractFace(self,image):\n",
    "      results = self.face_mesh.process(image)\n",
    "\n",
    "      if not results.multi_face_landmarks:\n",
    "        return None\n",
    "      return results.multi_face_landmarks[0]\n",
    "\n",
    "  def showSegmentedBody(self):\n",
    "    if self.debuging:\n",
    "      condition = np.stack((self.info[\"body_segmentation\"],) * 3, axis=-1) > 0.1\n",
    "      bg_image = np.zeros(self.image.shape, dtype=np.uint8)\n",
    "      bg_image[:] = self.BG_COLOR\n",
    "      annotated_image = np.where(condition, self.image, bg_image)\n",
    "      plt.imshow(annotated_image)\n",
    "      plt.show()\n",
    "    else:\n",
    "      print(\"enable debuging\")\n",
    "\n",
    "  def getSegmentedImage(self,info,image):\n",
    "      if  info['body_segmentation'] is not None:\n",
    "          condition = np.stack((info[\"body_segmentation\"],) * 3, axis=-1) > 0.1\n",
    "          bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "          bg_image[:] = self.BG_COLOR\n",
    "          annotated_image = np.where(condition, image, bg_image)\n",
    "          return annotated_image\n",
    "      else:\n",
    "          print(\"there is no segmented image for this frame\")\n",
    "          return list(np.zeros((512,512,3)))\n",
    "\n",
    "  def showLandmarks(self,info,image):\n",
    "    image_with_landmarks = image.copy()\n",
    "    if info[\"pose_landmarks\"] != None:\n",
    "\n",
    "      mp_drawing.draw_landmarks(\n",
    "              image_with_landmarks,\n",
    "              info[\"pose_landmarks\"],\n",
    "              mp_pose.POSE_CONNECTIONS,\n",
    "              landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    if info[\"hands_landmarks\"]['landmarks'] != None:\n",
    "      for hand_landmarks in info[\"hands_landmarks\"]['landmarks']:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_with_landmarks,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    if info[\"face_landmarks\"] != None:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image_with_landmarks,\n",
    "            landmark_list=info[\"face_landmarks\"],\n",
    "            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image_with_landmarks,\n",
    "            landmark_list=info[\"face_landmarks\"],\n",
    "            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image_with_landmarks,\n",
    "            landmark_list=info[\"face_landmarks\"],\n",
    "            connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_iris_connections_style())\n",
    "    plt.imshow(image_with_landmarks)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.578853Z",
     "iopub.status.busy": "2025-07-05T08:35:04.578567Z",
     "iopub.status.idle": "2025-07-05T08:35:04.596171Z",
     "shell.execute_reply": "2025-07-05T08:35:04.595482Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.578834Z"
    },
    "id": "us9F7kb9LKBB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCropper():\n",
    "  def __init__(self,debuging,target_size = (512, 512) ):\n",
    "    self.target_size=target_size\n",
    "    self.margin=20\n",
    "    self.debuging= debuging\n",
    "    self.transform=None\n",
    "    self.normalized=False\n",
    "    if debuging:\n",
    "      self.cropped_image = None\n",
    "\n",
    "  def cropImage(self,image, landmarks, checker):\n",
    "    if torch.is_tensor(image):\n",
    "        image = image.cpu().numpy()\n",
    "        image = (image).astype(np.uint8)\n",
    "    if torch.is_tensor(landmarks):\n",
    "        landmarks_np = landmarks.cpu().numpy()\n",
    "    else:\n",
    "        landmarks_np = np.array(landmarks).copy()\n",
    "\n",
    "    # Ensure landmarks shape is (N, 2)\n",
    "    if landmarks_np.ndim == 1 and landmarks_np.shape[0] % 2 == 0:\n",
    "        landmarks_np = landmarks_np.reshape(-1, 2)\n",
    "    elif landmarks_np.ndim != 2 or landmarks_np.shape[1] != 2:\n",
    "        raise ValueError(f\"Invalid landmarks shape: {landmarks_np.shape}\")\n",
    "\n",
    "    frame = image\n",
    "    lm = landmarks_np\n",
    "    if not checker['face_landmarks']:\n",
    "        variable = lm[6+21]\n",
    "        variable[1]-=50\n",
    "        lm[54:532,:] = variable\n",
    "\n",
    "\n",
    "    if not checker['right_hand_landmarks']:\n",
    "        lm[:21,:] = lm[17+21]\n",
    "\n",
    "    if not checker['left_hand_landmarks']:\n",
    "        lm[532:,:] = lm[18+21]\n",
    "    # print(\"this in cropped image\")\n",
    "    # for i in range(len(lm[21:54,:])):\n",
    "    #       print(\"index: \",i,\") pose: \",lm[21:54,:][i])\n",
    "\n",
    "    cropped_image, (x_min, x_max, y_min, y_max) = self.detect_and_crop_person(frame, lm, margin=self.margin)\n",
    "    if cropped_image.size == 0:\n",
    "        adjusted_landmarks = np.zeros_like(landmarks_np)\n",
    "        output_image = torch.zeros(3, self.target_size[1], self.target_size[0])\n",
    "        return output_image, adjusted_landmarks\n",
    "\n",
    "    processed_img, scale, (x_offset, y_offset) = self.resize_and_pad(cropped_image, target_size=self.target_size)\n",
    "\n",
    "    if self.normalized:\n",
    "        h, w, _ = frame.shape\n",
    "        x_coords = lm[:, 0] * w\n",
    "        y_coords = lm[:, 1] * h\n",
    "    else:\n",
    "        x_coords = lm[:, 0]\n",
    "        y_coords = lm[:, 1]\n",
    "\n",
    "    adjusted_x = (x_coords - x_min) * scale + x_offset\n",
    "    adjusted_y = (y_coords - y_min) * scale + y_offset\n",
    "\n",
    "    target_w, target_h = self.target_size\n",
    "    # out_of_bounds = (\n",
    "    #     (adjusted_x < 0) | (adjusted_x >= target_w) |\n",
    "    #     (adjusted_y < 0) | (adjusted_y >= target_h)\n",
    "    # )\n",
    "    # adjusted_x[out_of_bounds] = 0.0\n",
    "    # adjusted_y[out_of_bounds] = 0.0\n",
    "\n",
    "    if self.normalized:\n",
    "        adjusted_x /= target_w\n",
    "        adjusted_y /= target_h\n",
    "\n",
    "    adjusted_landmarks = np.stack([adjusted_x, adjusted_y], axis=-1)\n",
    "\n",
    "    if self.transform:\n",
    "        processed_img = self.transform(processed_img)\n",
    "    else:\n",
    "        processed_img = torch.from_numpy(processed_img).permute(2, 0, 1).float()\n",
    "    if self.debuging:\n",
    "      self.cropped_image = processed_img\n",
    "    return processed_img, adjusted_landmarks\n",
    "\n",
    "  def resize_and_pad(self,image, target_size=(512, 512)):\n",
    "      target_w, target_h = target_size\n",
    "      h, w, _ = image.shape\n",
    "      if h == 0 or w == 0:\n",
    "          return np.zeros((target_h, target_w, 3), dtype=np.uint8), 1.0, (0, 0)\n",
    "      padded_image = np.zeros((target_h, target_w, 3), dtype=np.uint8)\n",
    "      scale = min(target_w / w, target_h / h)\n",
    "      new_w, new_h = int(w * scale), int(h * scale)\n",
    "      if new_w == 0 or new_h == 0:\n",
    "          return padded_image, 1.0, (0, 0)\n",
    "      resized_image = cv2.resize(image, (new_w, new_h))\n",
    "      x_offset = (target_w - new_w) // 2\n",
    "      y_offset = (target_h - new_h) // 2\n",
    "      padded_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_image\n",
    "      return padded_image, scale, (x_offset, y_offset)\n",
    "\n",
    "  def detect_and_crop_person(self,frame, lm, margin=10):\n",
    "      h, w, _ = frame.shape\n",
    "      if self.normalized:\n",
    "          x_coords = lm[:, 0] * w\n",
    "          y_coords = lm[:, 1] * h\n",
    "      else:\n",
    "          x_coords = lm[:, 0]\n",
    "          y_coords = lm[:, 1]\n",
    "      x_min = int(max(np.min(x_coords) - margin, 0))\n",
    "      x_max = int(min(np.max(x_coords) + margin, w))\n",
    "      y_min = int(max(np.min(y_coords) - margin, 0))\n",
    "      y_max = int(min(np.max(y_coords) + margin, h))\n",
    "      if x_max <= x_min or y_max <= y_min:\n",
    "          return frame, (0, w, 0, h)\n",
    "      crop = frame[y_min:y_max, x_min:x_max]\n",
    "      if crop.size == 0:\n",
    "          return frame, (0, w, 0, h)\n",
    "      return crop, (x_min, x_max, y_min, y_max)\n",
    "  def showImage(self):\n",
    "    print(\"this cropped image: \")\n",
    "    image = self.cropped_image.permute(1,2,0).numpy().astype(dtype=np.uint16)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.597327Z",
     "iopub.status.busy": "2025-07-05T08:35:04.597002Z",
     "iopub.status.idle": "2025-07-05T08:35:04.625082Z",
     "shell.execute_reply": "2025-07-05T08:35:04.624352Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.597285Z"
    },
    "id": "MRfMLe8KLKBC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeaturesExtractorFromImage():\n",
    "  def __init__(self,debuging):\n",
    "    if debuging:\n",
    "      self.info = {\n",
    "          \"left_hand_image\":None,\n",
    "          \"right_hand_image\":None,\n",
    "          \"vertical\":None,\n",
    "          \"horizontal\":None,\n",
    "          \"left_distance\":None,\n",
    "          \"right_distance\":None\n",
    "      }\n",
    "\n",
    "    self.debuging= debuging\n",
    "    pass\n",
    "\n",
    "  def extractFeatures(self,image, landmarks_checker, scaled_landmarks):\n",
    "\n",
    "    if not landmarks_checker[\"pose_landmarks\"] and not landmarks_checker[\"right_hand_landmarks\"] and not landmarks_checker[\"left_hand_landmarks\"] and not landmarks_checker[\"face_landmarks\"]:\n",
    "      return list(np.zeros((39514)))\n",
    "    scaled_landmarks = scaled_landmarks\n",
    "    # right hands landmarks:  21\n",
    "    # pose landmarks:  33\n",
    "    # face landmarks:  478\n",
    "    # left hands landmarks:  21\n",
    "    right_hand_landmarks,pose_landmarks,face_landmarks,left_hand_landmarks = scaled_landmarks[:21,:],scaled_landmarks[21:54,:],scaled_landmarks[54:532,:],scaled_landmarks[532:,:]\n",
    "    # print(\"this now in mediapipe\")\n",
    "    # for i in range(len(scaled_landmarks[21:54,:])):\n",
    "    #       print(\"index: \",i,\") pose: \",scaled_landmarks[21:54,:][i])\n",
    "\n",
    "    vertical_regoins, horizontal_regions, right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations =self.handsRegoins(\n",
    "      landmarks_checker,\n",
    "      right_hand_landmarks,\n",
    "      pose_landmarks,\n",
    "      face_landmarks,\n",
    "      left_hand_landmarks,\n",
    "      image\n",
    "      )\n",
    "\n",
    "    left_distance,right_distance,left_hand_image, right_hand_image = self.getHandsDistanceFromFace(\n",
    "      landmarks_checker,\n",
    "      right_hands_box_cordinations,\n",
    "      pose_box_cordinations,\n",
    "      face_box_cordinations,\n",
    "      left_hands_box_cordinations,\n",
    "      image\n",
    "      )\n",
    "\n",
    "    LR_hand_regions = np.array([vertical_regoins,horizontal_regions])\n",
    "\n",
    "    # print(left_hand_image.reshape(-1).shape)\n",
    "    # print(right_hand_image.reshape(-1).shape)\n",
    "    # print(LR_hand_regions.reshape(-1).shape)\n",
    "    # print(left_distance.reshape(-1).shape)\n",
    "    # print(right_distance.reshape(-1).shape)\n",
    "    # print(right_hand_landmarks.reshape(-1).shape)\n",
    "    # print(pose_landmarks.reshape(-1).shape)\n",
    "    # print(face_landmarks.reshape(-1).shape)\n",
    "    # print(left_hand_landmarks.reshape(-1).shape)\n",
    "\n",
    "    # print(\"maximum inside extract functino: \",left_hand_image.max())\n",
    "    # plt.imshow( left_hand_image)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    frame_features = np.concatenate([left_hand_image.reshape(-1),right_hand_image.reshape(-1),LR_hand_regions.reshape(-1),left_distance.reshape(-1),right_distance.reshape(-1), right_hand_landmarks.reshape(-1),pose_landmarks.reshape(-1),face_landmarks.reshape(-1),left_hand_landmarks.reshape(-1)])\n",
    "    return torch.tensor(frame_features)\n",
    "\n",
    "  def getHandsDistanceFromFace(self,landmarks_checker,right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations, image):\n",
    "      margin = 10\n",
    "\n",
    "      left_hand_image= image[left_hands_box_cordinations[0][1]-margin:left_hands_box_cordinations[1][1]+margin,\n",
    "                        left_hands_box_cordinations[0][0]-margin:left_hands_box_cordinations[1][0]+margin]\n",
    "\n",
    "      right_hand_image= image[right_hands_box_cordinations[0][1]-margin:right_hands_box_cordinations[1][1]+margin,\n",
    "                        right_hands_box_cordinations[0][0]-margin:right_hands_box_cordinations[1][0]+margin]\n",
    "      left_hand_image = cv2.resize(left_hand_image, (80,80)) if left_hand_image.shape[0] > 0 and left_hand_image.shape[1] > 0 else np.zeros((80,80,3))\n",
    "      right_hand_image = cv2.resize(right_hand_image, (80,80)) if right_hand_image.shape[0] > 0 and right_hand_image.shape[1] > 0 else np.zeros((80,80,3))\n",
    "      distance_between_left_face = np.array([np.abs(face_box_cordinations[0][0]-left_hands_box_cordinations[0][0]),np.abs(face_box_cordinations[1][1] -left_hands_box_cordinations[1][1])])\n",
    "      distance_between_right_face = np.array([np.abs(face_box_cordinations[0][0]-right_hands_box_cordinations[0][0]),np.abs(face_box_cordinations[1][1] -right_hands_box_cordinations[1][1])])\n",
    "      if self.debuging:\n",
    "        self.info['left_distance'] = distance_between_left_face\n",
    "        self.info['right_distance'] = distance_between_right_face\n",
    "        self.info['left_hand_image'] = left_hand_image\n",
    "        self.info['right_hand_image'] = right_hand_image\n",
    "      return distance_between_left_face,distance_between_right_face,(left_hand_image),(right_hand_image)\n",
    "\n",
    "\n",
    "  def handsRegoins(self, landmarks_checker, right_hand_landmarks,pose_landmarks,face_landmarks,left_hand_landmarks,image):\n",
    "    right_hands_box_cordinations = np.min(right_hand_landmarks,axis = 0).astype(int), np.max(right_hand_landmarks,axis=0).astype(int)#min bottom left - max top right\n",
    "    pose_box_cordinations = np.min(pose_landmarks,axis = 0).astype(int), np.max(pose_landmarks,axis=0).astype(int)#min bottom left - max top right\n",
    "    face_box_cordinations = np.min(face_landmarks,axis = 0).astype(int), np.max(face_landmarks,axis=0).astype(int)#min bottom left - max top right\n",
    "    left_hands_box_cordinations = np.min(left_hand_landmarks,axis = 0).astype(int), np.max(left_hand_landmarks,axis=0).astype(int)#min bottom left - max top right\n",
    "\n",
    "    if not landmarks_checker['face_landmarks']:\n",
    "      new_box_cordinations = np.array([pose_landmarks[8],pose_landmarks[7],pose_landmarks[10],pose_landmarks[9]])\n",
    "      face_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n",
    "\n",
    "    if not landmarks_checker[\"right_hand_landmarks\"]:\n",
    "      new_box_cordinations = np.array([pose_landmarks[21],pose_landmarks[19],pose_landmarks[15],pose_landmarks[17]])\n",
    "      right_hands_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n",
    "    if not landmarks_checker[\"left_hand_landmarks\"]:\n",
    "      new_box_cordinations = np.array([pose_landmarks[20],pose_landmarks[22],pose_landmarks[18],pose_landmarks[16]])\n",
    "      left_hands_box_cordinations = np.min(new_box_cordinations,axis = 0).astype(int),np.max(new_box_cordinations,axis=0).astype(int)\n",
    "\n",
    "    vertical_hands_locations =[]\n",
    "    for hand in [left_hands_box_cordinations,right_hands_box_cordinations]:\n",
    "      if hand[0][1] <=face_box_cordinations[0][1]:\n",
    "        vertical_hands_locations.append(1)\n",
    "      elif hand[0][1] >=face_box_cordinations[0][1] and hand[0][1] < pose_box_cordinations[0][1]:\n",
    "        vertical_hands_locations.append(2)\n",
    "      elif hand[0][1] <face_box_cordinations[1][1] and hand[0][1] >= pose_box_cordinations[0][1]:\n",
    "        vertical_hands_locations.append(3)\n",
    "      elif hand[0][1] >pose_box_cordinations[0][1] and hand[0][1]>= face_box_cordinations[1][1] and hand[0][1]< (pose_box_cordinations[0][1]/2)+pose_box_cordinations[0][1] :\n",
    "        vertical_hands_locations.append(4)\n",
    "      else:\n",
    "        vertical_hands_locations.append(5)\n",
    "    if self.debuging:\n",
    "      print(\"this in mediapipe\")\n",
    "      m = image.copy()\n",
    "      top_left, bottom_right = left_hands_box_cordinations\n",
    "      cx = int((top_left[0] + bottom_right[0]) / 2)\n",
    "      cy = int((top_left[1] + bottom_right[1]) / 2)\n",
    "      center = (cx, cy)\n",
    "      cv2.circle(m, center, radius=5, color=(0, 0, 255), thickness=-1)\n",
    "      cv2.rectangle(m, tuple(face_box_cordinations[0]), tuple(face_box_cordinations[1]), (255, 0, 0), 2)\n",
    "      cv2.rectangle(m, tuple(left_hands_box_cordinations[0]), tuple(left_hands_box_cordinations[1]), (255, 0, 255), 2)\n",
    "      cv2.rectangle(m, tuple(right_hands_box_cordinations[0]), tuple(right_hands_box_cordinations[1]), (255, 255, 0), 2)\n",
    "      cv2.rectangle(m, tuple(pose_box_cordinations[0]), tuple(pose_box_cordinations[1]), (255, 255, 255), 2)\n",
    "      print(\"this is tuple: \",tuple(pose_box_cordinations[0]), tuple(pose_box_cordinations[1]))\n",
    "      # for i in range(len(pose_landmarks)):\n",
    "      #     print(\"index: \",i,\") pose: \",pose_landmarks[i])\n",
    "      plt.imshow(m)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    horizontal_hands_locations = []\n",
    "    for hand in [left_hands_box_cordinations,right_hands_box_cordinations]:\n",
    "      if hand[0][0]< pose_box_cordinations[0][0]:\n",
    "        horizontal_hands_locations.append(1)\n",
    "      elif hand[0][0] >= pose_box_cordinations[0][0] and hand[0][0] < face_box_cordinations[0][0]:\n",
    "        horizontal_hands_locations.append(2)\n",
    "      elif hand[0][0] >= face_box_cordinations[0][0] and hand[0][0] < face_box_cordinations[1][0]:\n",
    "        horizontal_hands_locations.append(3)\n",
    "      elif hand[0][0] >=face_box_cordinations[1][0] and hand[0][0]< pose_box_cordinations[1][0]:\n",
    "        horizontal_hands_locations.append(4)\n",
    "      else:\n",
    "        horizontal_hands_locations.append(5)\n",
    "    if self.debuging:\n",
    "      self.info['horizontal'] = horizontal_hands_locations\n",
    "      self.info['vertical'] = vertical_hands_locations\n",
    "    return vertical_hands_locations,horizontal_hands_locations, right_hands_box_cordinations, pose_box_cordinations, face_box_cordinations, left_hands_box_cordinations\n",
    "\n",
    "  def showFeatures(self):\n",
    "    print(\"left hand distance from the face: \", self.info['left_distance'])\n",
    "    print(\"right hand distance from the face: \", self.info['right_distance'])\n",
    "    print(\"vertical hands regions: \",self.info[\"vertical\"])\n",
    "    print(\"horizontal hands regions: \",self.info[\"horizontal\"])\n",
    "    print(\"left hand image: \")\n",
    "    plt.imshow(self.info[\"left_hand_image\"])\n",
    "    plt.show()\n",
    "    print(\"right hand image: \")\n",
    "    plt.imshow(self.info[\"right_hand_image\"])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.626721Z",
     "iopub.status.busy": "2025-07-05T08:35:04.625900Z",
     "iopub.status.idle": "2025-07-05T08:35:04.646929Z",
     "shell.execute_reply": "2025-07-05T08:35:04.646323Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.626684Z"
    },
    "id": "9AatD0V8LKBC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MediapipeFeaturesExtractor():\n",
    "  def __init__(self,debuging =True,target_size = (512,512)):\n",
    "    self.imageCropper = ImageCropper(debuging,target_size )\n",
    "    self.featuresExtractorFromImage = FeaturesExtractorFromImage(debuging)\n",
    "    self.debuging = debuging\n",
    "\n",
    "\n",
    "  def extractFeaturesFromVideo(self, frames, landmarks, segmented_frames):\n",
    "    cropped_images = []\n",
    "    mediapipeFeatures = []\n",
    "    updated_adjusted_landmarks = []\n",
    "    previous_scaled_up_right_hand_landmarks = [0]\n",
    "    previous_scaled_up_pose_landmarks = [0]\n",
    "    previous_scaled_up_face_landmarks = [0]\n",
    "    previous_scaled_up_left_hand_landmarks = [0]\n",
    "\n",
    "    # for frame in tqdm(frames):\n",
    "    #   self.extractor.extractLandmarks(frame)\n",
    "    #   checkers.append(self.extractor.check())\n",
    "    #   fframes.append(self.extractor.image)\n",
    "    #   landmarks.append(self.extractor.get_x_y_landmarks_scaled_up())\n",
    "    for frame,landmark,segmented_frame in zip(frames,landmarks, segmented_frames):\n",
    "\n",
    "      checker = {\n",
    "          \"right_hand_landmarks\":(landmark[-2][0]==1),\n",
    "          \"left_hand_landmarks\":(landmark[-1][1]==1),\n",
    "          \"pose_landmarks\":(landmark[-2][1]==1),\n",
    "          \"face_landmarks\":(landmark[-1][0]==1),\n",
    "      }\n",
    "      if not checker['right_hand_landmarks'] and np.sum(previous_scaled_up_right_hand_landmarks) != 0 :\n",
    "        landmark[:21,:] = previous_scaled_up_right_hand_landmarks\n",
    "\n",
    "      if not checker['left_hand_landmarks'] and np.sum(previous_scaled_up_right_hand_landmarks) != 0 :\n",
    "        landmark[532:-2,:] = previous_scaled_up_left_hand_landmarks\n",
    "\n",
    "      if not checker['pose_landmarks'] and np.sum(previous_scaled_up_pose_landmarks) != 0:\n",
    "        landmark[21:54,:]= previous_scaled_up_pose_landmarks\n",
    "      if not checker['face_landmarks'] and np.sum(previous_scaled_up_face_landmarks) != 0:\n",
    "        landmark[54:532,:]= previous_scaled_up_face_landmarks\n",
    "\n",
    "      previous_scaled_up_right_hand_landmarks = landmark[:21,:]\n",
    "      previous_scaled_up_pose_landmarks = landmark[21:54,:]\n",
    "      previous_scaled_up_face_landmarks = landmark[54:532,:]\n",
    "      previous_scaled_up_left_hand_landmarks = landmark[532:-2,:]\n",
    "\n",
    "      cropped_image, _ = self.imageCropper.cropImage(segmented_frame, landmark[:-2], checker)\n",
    "      new_frame, adjusted_landmark = self.imageCropper.cropImage(frame, landmark[:-2], checker)\n",
    "      mediapipeFeature = self.featuresExtractorFromImage.extractFeatures(new_frame.permute(1,2,0).numpy().astype(dtype=np.uint16),checker,adjusted_landmark)\n",
    "\n",
    "      updated_adjusted_landmarks.append(torch.tensor(adjusted_landmark))\n",
    "      cropped_images.append(cropped_image)\n",
    "      mediapipeFeatures.append(torch.tensor(mediapipeFeature))\n",
    "      if self.debuging:\n",
    "        self.showImages()\n",
    "    return torch.stack(cropped_images), torch.stack(mediapipeFeatures), torch.stack(updated_adjusted_landmarks)\n",
    "\n",
    "  def showImages(self):\n",
    "    if self.debuging:\n",
    "      self.imageCropper.showImage()\n",
    "      self.featuresExtractorFromImage.showFeatures()\n",
    "    else:\n",
    "      print(\"sett debuging flag to True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.650503Z",
     "iopub.status.busy": "2025-07-05T08:35:04.650255Z",
     "iopub.status.idle": "2025-07-05T08:35:04.665709Z",
     "shell.execute_reply": "2025-07-05T08:35:04.665132Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.650488Z"
    },
    "id": "XVWSKSSoLKBC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProcessVideo():\n",
    "  def __init__(self,debuging=False,debuging_landmarks=False,target_size=(512,512)):\n",
    "    self.mediapipeFeaturesExtractor = MediapipeFeaturesExtractor(debuging,target_size)\n",
    "    self.extractor = Extractor(debuging, target_size)\n",
    "    self.debuging = debuging\n",
    "\n",
    "  def extractLandmarks(self, frames):\n",
    "      landmarks, segmented_image, image = self.extractor.extractLandmarks(frames)\n",
    "      return landmarks, segmented_image, image\n",
    "\n",
    "  def extractFeaturesFromVideo(self, frames, landmarks, segmented_frames):\n",
    "    cropped_images, mediapipeFeatures, updated_adjusted_landmarks = self.mediapipeFeaturesExtractor.extractFeaturesFromVideo(frames, landmarks, segmented_frames)\n",
    "    return cropped_images, mediapipeFeatures, updated_adjusted_landmarks\n",
    " # ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.666537Z",
     "iopub.status.busy": "2025-07-05T08:35:04.666372Z",
     "iopub.status.idle": "2025-07-05T08:35:04.686785Z",
     "shell.execute_reply": "2025-07-05T08:35:04.686136Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.666524Z"
    },
    "id": "ZYtswGoBLKBC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self,prediciton_dictanory,model_weights_path=None,boundary_model_weights_path=None,num_partitions= 5,debuging=False):\n",
    "       self.prediciton_dictanory = prediciton_dictanory\n",
    "       self.model = Model().to(device)\n",
    "       self.boundaryDetector = BoundaryDetector().to(device)\n",
    "\n",
    "       if model_weights_path != None:\n",
    "           if model_weights_path.endswith(\"checkpoint.pth\"):\n",
    "              if device ==torch.device('cpu'):\n",
    "                  checkpoint = torch.load(model_weights_path, map_location=torch.device('cpu'))\n",
    "                  self.model.load_state_dict(checkpoint[\"best_model_state_dict\"])\n",
    "              else:\n",
    "                  checkpoint = torch.load(model_weights_path)\n",
    "                  self.model.load_state_dict(checkpoint[\"best_model_state_dict\"])\n",
    "           else:\n",
    "              self.model.load_state_dict(torch.load(model_weights_path))\n",
    "       if boundary_model_weights_path != None:\n",
    "           if device ==torch.device('cpu'):\n",
    "              self.boundaryDetector.load_state_dict(torch.load(boundary_model_weights_path,map_location=torch.device('cpu')))\n",
    "           else:\n",
    "              self.boundaryDetector.load_state_dict(torch.load(boundary_model_weights_path))\n",
    "\n",
    "       self.num_partitions= num_partitions\n",
    "       self.debuging = debuging\n",
    "\n",
    "    def detectBoundary(self,poseLandmarks, max_seq=120*6):\n",
    "        train = list(poseLandmarks)\n",
    "        numberofFrames = len(poseLandmarks)\n",
    "        if numberofFrames < max_seq:\n",
    "          remaining_frames = max_seq - numberofFrames\n",
    "          train.extend(torch.zeros((remaining_frames,33,2)))\n",
    "        train = torch.stack(train)\n",
    "        train = train.to(device)\n",
    "        output = self.boundaryDetector(train.unsqueeze(0))\n",
    "        output = torch.argmax(output.squeeze(0), dim= -1)\n",
    "        print(output)\n",
    "        boundaries = [ii for ii,tt in enumerate(output) if tt==1]\n",
    "        return boundaries\n",
    "    ####\n",
    "    def predict(self,cropped_images, mediapipefeaturess):\n",
    "            self.model.eval()\n",
    "        # if len(cropped_images.shape) == 5:\n",
    "            seq_predictions = []\n",
    "            for croped_image,mediapipefeatures in tqdm(zip(cropped_images,mediapipefeaturess)):\n",
    "                # croped_image = torch.tensor(cropped_image)\n",
    "                # mediapipefeatures = torch.tensor(mediapipefeatures)\n",
    "\n",
    "                x_croped_image = []\n",
    "                x_mediapipefeatures = []\n",
    "\n",
    "                startingIndcies = torch.linspace(0,int(croped_image.shape[0]/2)-1,steps=self.num_partitions).long()\n",
    "\n",
    "                for start in startingIndcies:\n",
    "                    indcies = torch.linspace(start,int(croped_image.shape[0])-1,steps=32).long()\n",
    "                    x_croped_image.append(croped_image[indcies])\n",
    "                    x_mediapipefeatures.append(mediapipefeatures[indcies])\n",
    "\n",
    "                x_croped_image = torch.stack(x_croped_image).to(device)\n",
    "                x_mediapipefeatures = torch.stack(x_mediapipefeatures).to(device)\n",
    "                if self.debuging:\n",
    "                    for i in x_croped_image[0].cpu().permute(0,2,3,1):\n",
    "                        plt.imshow(i/255)\n",
    "                        plt.show()\n",
    "                    print(\"this is the end of the video\")\n",
    "                # print(\"cropped images shape: \", x_croped_image.shape)\n",
    "                # print(\"medipaipe features shape: \", x_mediapipefeatures.shape)\n",
    "                with torch.no_grad():\n",
    "                    mediapipefeatures = mediapipefeatures.float().to(device).unsqueeze(0)\n",
    "                    croped_image = croped_image.float().to(device).unsqueeze(0)\n",
    "                    probabilities = self.model(x_mediapipefeatures,x_croped_image)\n",
    "                    softmax_probs = F.softmax(probabilities, dim=1)\n",
    "                    top5_probs, top5_indices = torch.topk(softmax_probs, k=5, dim=1)\n",
    "\n",
    "                    # Print top-5 for each prediction in the batch\n",
    "                    for batch_idx in range(top5_indices.size(0)):\n",
    "                        print(f\"\\nTop-5 predictions for sample {batch_idx}:\")\n",
    "                        for rank in range(5):\n",
    "                            pred_idx = top5_indices[batch_idx][rank].item()\n",
    "                            pred_word = self.prediciton_dictanory[pred_idx]\n",
    "                            prob = top5_probs[batch_idx][rank].item()\n",
    "                            print(f\"  {rank+1}) {pred_word} - {prob*100:.2f}%\")\n",
    "\n",
    "                    # Still get argmax for majority voting\n",
    "                    encdoed_output = torch.argmax(probabilities, dim=1)\n",
    "                    predictions = [self.prediciton_dictanory[prediction.item()] for prediction in encdoed_output]\n",
    "\n",
    "                    dataDictanory = {word:0 for word in set(predictions)}\n",
    "                    for i in predictions:\n",
    "                      dataDictanory[i]+=1\n",
    "                    frequentLabel =predictions[0]\n",
    "                    frequency = 0\n",
    "                    for l,v in dataDictanory.items():\n",
    "                      if v>frequency:\n",
    "                        frequentLabel = l\n",
    "                        frequency = v\n",
    "                    seq_predictions.append(frequentLabel)\n",
    "            return seq_predictions\n",
    "\n",
    "        # else:\n",
    "        #         x_croped_image = []\n",
    "        #         x_mediapipefeatures = []\n",
    "\n",
    "\n",
    "        #         startingIndcies = torch.linspace(0,int(croped_image.size(0)/2)-1,steps=self.num_partitions).long()\n",
    "\n",
    "        #         for start in startingIndcies:\n",
    "        #             indcies = torch.linspace(start,int(croped_image.size(0))-1,steps=32).long()\n",
    "        #             x_croped_image.append(croped_image[indcies])\n",
    "        #             x_mediapipefeatures.append(mediapipefeatures[indcies])\n",
    "\n",
    "        #         x_croped_image = torch.stack(x_croped_image).to(device)\n",
    "        #         x_mediapipefeatures = torch.stack(x_mediapipefeatures).to(device)\n",
    "        #         # print(\"cropped images shape: \", x_croped_image.shape)\n",
    "        #         # print(\"medipaipe features shape: \", x_mediapipefeatures.shape)\n",
    "        #         with torch.no_grad():\n",
    "        #             mediapipefeatures = mediapipefeatures.float().to(device).unsqueeze(0)\n",
    "        #             croped_image = croped_image.float().to(device).unsqueeze(0)\n",
    "        #             probabilities = self.model(x_mediapipefeatures,x_croped_image)\n",
    "        #             encdoed_output = torch.argmax(probabilities,dim=1)\n",
    "        #             predictions =[self.prediciton_dictanory[prediction.item()] for prediction in encdoed_output]\n",
    "        #             dataDictanory = {word:0 for word in set(predictions)}\n",
    "        #             for i in predictions:\n",
    "        #               dataDictanory[i]+=1\n",
    "        #             frequentLabel =predictions[0]\n",
    "        #             frequency = 0\n",
    "        #             for l,v in dataDictanory.items():\n",
    "        #               if v>frequency:\n",
    "        #                 frequentLabel = l\n",
    "        #                 frequency = v\n",
    "\n",
    "        #         return frequentLabel, predictions\n",
    "# processVideo= ProcessVideo(True)\n",
    "# frames = processVideo.extractFrames(\"/kaggle/input/new-video-for-testing/WIN_20250609_13_14_53_Pro.mp4\")\n",
    "\n",
    "# _=processVideo.extractMediapipeFeatures(frames)\n",
    "# start = time.time()\n",
    "# _,_,_ = processVideo.extractor.extractLandmarks(frames)\n",
    "# time.time()-start\n",
    "# Image.fromarray(np.array(frames,dtype= np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.688256Z",
     "iopub.status.busy": "2025-07-05T08:35:04.687609Z",
     "iopub.status.idle": "2025-07-05T08:35:04.705475Z",
     "shell.execute_reply": "2025-07-05T08:35:04.704839Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.688237Z"
    },
    "id": "_8mDwtTaLKBD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UploadManager():\n",
    "    def __init__(self,prediciton_dictanory, model_weights_path=None,boundary_model_weights_path=None, num_partitions=5, extractOneFromEach= 2, intersectionRatio = 0.5, predict_after = 40,target_size=(512,512), debuging=False,debuging_landmarks=False,debuging_classifier= False):\n",
    "        self.target_size = target_size\n",
    "        self.extractOneFromEach = extractOneFromEach\n",
    "        self.predict_after = predict_after\n",
    "        self.intersectionRatio = intersectionRatio\n",
    "        self.transform = transforms.CenterCrop((512, 512))\n",
    "\n",
    "        self.mediapipeFeatures =[]\n",
    "        self.cropedImages = []\n",
    "\n",
    "        self.processVideo = ProcessVideo(debuging=debuging,debuging_landmarks=debuging_landmarks,target_size=(512,512))\n",
    "        self.classifier = Classifier(prediciton_dictanory,model_weights_path=model_weights_path,boundary_model_weights_path= boundary_model_weights_path,num_partitions= 5,debuging= debuging_classifier)\n",
    "\n",
    "    def checkLandmarks(self,frame):\n",
    "        checker = self.processVideo.extractor.extractLandmarks(np.array(frame))\n",
    "        return checker\n",
    "    def extractFrames(self,video_path, video=False):\n",
    "      frames = []\n",
    "      valid_video=True\n",
    "      cap = cv2.VideoCapture(video_path)\n",
    "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "      if fps == 0:\n",
    "        valid_video=False\n",
    "        print(\"End of video or error occurred.\")\n",
    "        return None\n",
    "\n",
    "      while True:\n",
    "        ret, frame = cap.read()\n",
    "        valid_video=True\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # plt.imshow(frame)\n",
    "        # plt.show()\n",
    "        # Resize the frame to the target size before appending\n",
    "        if not video:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = self.transform(frame)\n",
    "        # plt.imshow(frame)\n",
    "        # plt.show()\n",
    "        frames.append(frame)\n",
    "\n",
    "      if valid_video:\n",
    "        frames = np.array(frames, dtype=np.uint8)\n",
    "\n",
    "        tensor = torch.tensor(frames)\n",
    "\n",
    "        # transformed_images = [self.top_center_crop_cv2(image) for image in tensor]\n",
    "\n",
    "        return tensor\n",
    "      else:\n",
    "        print(\"there is error in the frames\")\n",
    "        return None\n",
    "\n",
    "    ###\n",
    "    def predict(self, frames):\n",
    "        landmarks, segmented_frames, frames =self.processVideo.extractLandmarks(np.array(frames))\n",
    "        print(\"landmarks extracted successfully \")\n",
    "        landmarks = np.array(landmarks)\n",
    "        frames = np.array(frames)\n",
    "        segmented_frames = np.array(segmented_frames)\n",
    "        cropped_images, mediapipeFeatures, updated_adjusted_landmarks = self.processVideo.extractFeaturesFromVideo( frames, landmarks, segmented_frames)\n",
    "        print(\"mediapipe features extracted successfully \")\n",
    "\n",
    "        pose_landmarks = updated_adjusted_landmarks[:,21:54,:].clone().float()\n",
    "        boundaries_dash = self.classifier.detectBoundary(pose_landmarks)\n",
    "        print(\"boundaries is: \", boundaries_dash)\n",
    "\n",
    "        boundaries = []\n",
    "        for tt in range(len(boundaries_dash)):\n",
    "          if tt!= 0 and boundaries_dash[tt]== boundaries_dash[tt-1]:\n",
    "            continue\n",
    "          else:\n",
    "            boundaries.append(boundaries_dash[tt])\n",
    "\n",
    "        def remove_consecutive_increases(lst):\n",
    "            if not lst:\n",
    "                return []\n",
    "\n",
    "            result = [lst[0]]\n",
    "\n",
    "            for i in range(1, len(lst)):\n",
    "                if lst[i] != lst[i-1] + 1:\n",
    "                    result.append(lst[i])\n",
    "\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "        boundaries = remove_consecutive_increases(boundaries)\n",
    "\n",
    "        print(boundaries)\n",
    "\n",
    "        print(cropped_images.shape)\n",
    "        cropped_images_compo = []\n",
    "        mediapipeFeatures_compo = []\n",
    "        start = 0\n",
    "        for end in boundaries:\n",
    "            end = int(end)\n",
    "            cropped_images_compo.append(cropped_images[start:end, :, :, :])\n",
    "            mediapipeFeatures_compo.append(mediapipeFeatures[start:end, :])\n",
    "            start = end\n",
    "        cropped_images_compo = cropped_images_compo\n",
    "        mediapipeFeatures_compo = mediapipeFeatures_compo\n",
    "        print(\"generated sequences successfully \")\n",
    "\n",
    "        predictions = self.classifier.predict(cropped_images_compo, mediapipeFeatures_compo)\n",
    "        print(\"predictions successfully \")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa7U0WZlLKBD"
   },
   "source": [
    " [43, 196, 197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.706356Z",
     "iopub.status.busy": "2025-07-05T08:35:04.706084Z",
     "iopub.status.idle": "2025-07-05T08:35:04.722274Z",
     "shell.execute_reply": "2025-07-05T08:35:04.721713Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.706340Z"
    },
    "id": "XSyPmon9h7_4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# uploadManager = UploadManager(\n",
    "#         prediciton_dictanory={0: 'always',\n",
    "#   1: 'apologize',\n",
    "#   2: 'bad',\n",
    "#   3: 'bring',\n",
    "#   4: 'drink',\n",
    "#   5: 'eat',\n",
    "#   6: 'friend',\n",
    "#   7: 'get_well',\n",
    "#   8: 'good',\n",
    "#   9: 'i',\n",
    "#   10: 'never',\n",
    "#   11: 'thanks',\n",
    "#   12: 'time',\n",
    "#   13: 'tomorrow',\n",
    "#   14: 'wait',\n",
    "#   15: 'where',\n",
    "#   16: 'who',\n",
    "#   17: 'why',\n",
    "#   18: 'yesterday',\n",
    "#   19: 'you'},\n",
    "#         model_weights_path=model_path,\n",
    "        # boundary_model_weights_path= boundaryModelPath,\n",
    "        # num_partitions=5,\n",
    "        # extractOneFromEach= 2,\n",
    "        # intersectionRatio = 0.9,\n",
    "        # predict_after = 60,\n",
    "        # target_size=(512,512),\n",
    "        # debuging=False,\n",
    "        # debuging_landmarks=False,\n",
    "        # debuging_classifier = False\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.723399Z",
     "iopub.status.busy": "2025-07-05T08:35:04.723045Z",
     "iopub.status.idle": "2025-07-05T08:35:04.739336Z",
     "shell.execute_reply": "2025-07-05T08:35:04.738524Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.723378Z"
    },
    "id": "Tf_4Q00SLKBE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# frames =uploadManager.extractFrames(\"/content/you.mp4\")\n",
    "# # # frames =list(frames)\n",
    "# # # frames.extend(frames)\n",
    "# # # frames.extend(frames)\n",
    "# # # frames = torch.stack(frames)\n",
    "# print(\"number of frames: \",len(frames))\n",
    "# prediction = uploadManager.predict(frames)\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.740407Z",
     "iopub.status.busy": "2025-07-05T08:35:04.740125Z",
     "iopub.status.idle": "2025-07-05T08:35:04.755665Z",
     "shell.execute_reply": "2025-07-05T08:35:04.754971Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.740392Z"
    },
    "id": "aHyBMu-GLKBF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i in frames[120:196]:\n",
    "#     plt.imshow(i)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:04.756573Z",
     "iopub.status.busy": "2025-07-05T08:35:04.756392Z",
     "iopub.status.idle": "2025-07-05T08:35:09.030717Z",
     "shell.execute_reply": "2025-07-05T08:35:09.029956Z",
     "shell.execute_reply.started": "2025-07-05T08:35:04.756556Z"
    },
    "id": "7WKg07RoLKBF",
    "outputId": "797afb63-a86a-4645-8786-ead573939e74",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
      "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (11.1.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (7.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->imageio[ffmpeg]) (2.4.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->imageio[ffmpeg]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->imageio[ffmpeg]) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->imageio[ffmpeg]) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->imageio[ffmpeg]) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->imageio[ffmpeg]) (2024.2.0)\n",
      "Downloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uvicorn, python-multipart, pyngrok, starlette, fastapi\n",
      "Successfully installed fastapi-0.115.14 pyngrok-7.2.11 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn nest_asyncio pyngrok python-multipart imageio[ffmpeg] imageio-ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:09.031939Z",
     "iopub.status.busy": "2025-07-05T08:35:09.031711Z",
     "iopub.status.idle": "2025-07-05T08:35:11.738691Z",
     "shell.execute_reply": "2025-07-05T08:35:11.738022Z",
     "shell.execute_reply.started": "2025-07-05T08:35:09.031916Z"
    },
    "id": "EtL8VGuOLKBF",
    "outputId": "8c69b7df-c07c-40e3-f1e6-7de21b1c07d7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Your URL:                                                                                   \n",
      " NgrokTunnel: \"https://b8ad-34-73-123-51.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "from fastapi import FastAPI, File, UploadFile, WebSocket\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "\n",
    "ngrok.kill()  # Clean up previous ngrok tunnels\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "NGROK_AUTH_TOKEN = \"<YOUR_NGROK_AUTH_TOKEN>\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"This is Your URL: \\n\",public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:11.739733Z",
     "iopub.status.busy": "2025-07-05T08:35:11.739491Z",
     "iopub.status.idle": "2025-07-05T08:35:11.760076Z",
     "shell.execute_reply": "2025-07-05T08:35:11.759275Z",
     "shell.execute_reply.started": "2025-07-05T08:35:11.739708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boundary_model_weights (2).pth']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/kaggle/input/boundary-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-05T08:35:11.761223Z",
     "iopub.status.busy": "2025-07-05T08:35:11.760948Z"
    },
    "id": "7rYguU47LKBG",
    "outputId": "dd9ee41e-28f2-4752-eecc-38caadd88ca3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to /usr/local/lib/python3.11/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1751704511.942752     143 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751704511.981730     145 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751704511.986888     142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751704512.023920     142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751704512.036002     136 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751704512.085030     138 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 120MB/s]\n",
      "INFO:     Started server process [35]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]W0000 00:00:1751704797.169846     137 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "100%|██████████| 101/101 [00:08<00:00, 12.16it/s]\n",
      "/tmp/ipykernel_35/1745048269.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mediapipeFeatures.append(torch.tensor(mediapipeFeature))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [100]\n",
      "[100]\n",
      "torch.Size([101, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n",
      "  1) thanks - 99.98%\n",
      "  2) accident - 0.02%\n",
      "  3) police - 0.00%\n",
      "  4) get_well - 0.00%\n",
      "  5) apologize - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) thanks - 99.98%\n",
      "  2) accident - 0.02%\n",
      "  3) get_well - 0.00%\n",
      "  4) police - 0.00%\n",
      "  5) apologize - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) accident - 77.60%\n",
      "  2) thanks - 21.75%\n",
      "  3) get_well - 0.36%\n",
      "  4) where - 0.20%\n",
      "  5) forbidden - 0.05%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) thanks - 99.99%\n",
      "  2) get_well - 0.01%\n",
      "  3) accident - 0.00%\n",
      "  4) forbidden - 0.00%\n",
      "  5) where - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) thanks - 64.70%\n",
      "  2) get_well - 29.37%\n",
      "  3) accident - 5.50%\n",
      "  4) why - 0.20%\n",
      "  5) apologize - 0.07%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 57/57 [00:05<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [56]\n",
      "[56]\n",
      "torch.Size([57, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) breakfast - 0.00%\n",
      "  4) police - 0.00%\n",
      "  5) time - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) breakfast - 0.00%\n",
      "  4) police - 0.00%\n",
      "  5) time - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) breakfast - 0.00%\n",
      "  4) police - 0.00%\n",
      "  5) time - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) breakfast - 0.00%\n",
      "  4) police - 0.00%\n",
      "  5) time - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) police - 0.00%\n",
      "  4) breakfast - 0.00%\n",
      "  5) time - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 130/130 [00:11<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [62, 129]\n",
      "[62, 129]\n",
      "torch.Size([130, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) forbidden - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) bring - 0.00%\n",
      "  4) why - 0.00%\n",
      "  5) full - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) forbidden - 100.00%\n",
      "  2) why - 0.00%\n",
      "  3) tomorrow - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) bring - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) forbidden - 100.00%\n",
      "  2) bring - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) tomorrow - 0.00%\n",
      "  5) police - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) forbidden - 100.00%\n",
      "  2) why - 0.00%\n",
      "  3) bring - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) police - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) forbidden - 100.00%\n",
      "  2) why - 0.00%\n",
      "  3) police - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) police - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) full - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) tomorrow - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) forbidden - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) police - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) full - 0.00%\n",
      "  5) forbidden - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) tomorrow - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) tomorrow - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 96/96 [00:07<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [42, 43, 44, 95]\n",
      "[42, 95]\n",
      "torch.Size([96, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) accident - 99.40%\n",
      "  2) time - 0.41%\n",
      "  3) forbidden - 0.19%\n",
      "  4) sibling - 0.00%\n",
      "  5) get_well - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) accident - 99.97%\n",
      "  2) forbidden - 0.02%\n",
      "  3) time - 0.01%\n",
      "  4) sibling - 0.00%\n",
      "  5) get_well - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) accident - 100.00%\n",
      "  2) forbidden - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) apologize - 0.00%\n",
      "  5) same - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) always - 99.99%\n",
      "  2) forbidden - 0.01%\n",
      "  3) accident - 0.00%\n",
      "  4) same - 0.00%\n",
      "  5) where - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) belt - 100.00%\n",
      "  2) always - 0.00%\n",
      "  3) thanks - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) apologize - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) police - 99.88%\n",
      "  2) hurry - 0.11%\n",
      "  3) apologize - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) glove - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) police - 99.72%\n",
      "  2) hurry - 0.20%\n",
      "  3) apologize - 0.07%\n",
      "  4) sibling - 0.01%\n",
      "  5) glove - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) police - 99.99%\n",
      "  2) hurry - 0.01%\n",
      "  3) apologize - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) glove - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) police - 79.47%\n",
      "  2) hurry - 19.92%\n",
      "  3) glove - 0.51%\n",
      "  4) apologize - 0.06%\n",
      "  5) sibling - 0.03%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) police - 100.00%\n",
      "  2) hurry - 0.00%\n",
      "  3) friend - 0.00%\n",
      "  4) apologize - 0.00%\n",
      "  5) accident - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 2/41 [00:00<00:02, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 8/41 [00:00<00:02, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 12/41 [00:00<00:02, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 14/41 [00:01<00:02, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 20/41 [00:01<00:01, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 22/41 [00:01<00:01, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 26/41 [00:01<00:01, 12.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 30/41 [00:02<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 32/41 [00:02<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 36/41 [00:02<00:00, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 38/41 [00:02<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:03<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "landmarks extracted successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [24, 25, 28, 29, 30]\n",
      "[24, 28]\n",
      "torch.Size([41, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) bed - 71.10%\n",
      "  2) police - 26.36%\n",
      "  3) full - 0.80%\n",
      "  4) single - 0.43%\n",
      "  5) get_well - 0.37%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) bed - 71.54%\n",
      "  2) police - 22.62%\n",
      "  3) single - 3.27%\n",
      "  4) who - 0.60%\n",
      "  5) wait - 0.45%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) belt - 99.95%\n",
      "  2) friend - 0.04%\n",
      "  3) thanks - 0.01%\n",
      "  4) apologize - 0.00%\n",
      "  5) why - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) belt - 31.11%\n",
      "  2) wait - 23.78%\n",
      "  3) police - 18.51%\n",
      "  4) where - 9.86%\n",
      "  5) who - 6.97%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) where - 99.99%\n",
      "  2) wait - 0.01%\n",
      "  3) belt - 0.00%\n",
      "  4) who - 0.00%\n",
      "  5) bed - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) bed - 99.98%\n",
      "  2) police - 0.01%\n",
      "  3) full - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) bed - 99.98%\n",
      "  2) police - 0.01%\n",
      "  3) full - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) bed - 99.98%\n",
      "  2) police - 0.01%\n",
      "  3) full - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) bed - 99.98%\n",
      "  2) police - 0.01%\n",
      "  3) full - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) friend - 92.45%\n",
      "  2) bed - 7.28%\n",
      "  3) full - 0.10%\n",
      "  4) hurry - 0.05%\n",
      "  5) time - 0.04%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 80/200 [00:06<00:10, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 84/200 [00:07<00:09, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 89/200 [00:07<00:06, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 93/200 [00:07<00:06, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 97/200 [00:08<00:07, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:08<00:06, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 107/200 [00:08<00:06, 15.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 111/200 [00:08<00:05, 15.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 115/200 [00:09<00:05, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 117/200 [00:09<00:05, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [00:09<00:05, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 125/200 [00:09<00:04, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [00:10<00:03, 18.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 136/200 [00:10<00:03, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 138/200 [00:10<00:03, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 145/200 [00:10<00:02, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [00:11<00:02, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 153/200 [00:11<00:02, 20.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 159/200 [00:11<00:01, 23.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 165/200 [00:11<00:01, 26.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 171/200 [00:11<00:01, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 177/200 [00:12<00:00, 27.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 183/200 [00:12<00:00, 25.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 186/200 [00:12<00:00, 22.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 192/200 [00:12<00:00, 18.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 196/200 [00:13<00:00, 17.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:13<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no segmented image for this frame\n",
      "there is no segmented image for this frame\n",
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [9, 11, 21, 23, 24, 27, 29, 55, 56, 103, 104, 106, 107, 108, 114]\n",
      "[9, 11, 21, 23, 27, 29, 55, 103, 106, 114]\n",
      "torch.Size([200, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) bed - 100.00%\n",
      "  2) police - 0.00%\n",
      "  3) get_well - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) bed - 100.00%\n",
      "  2) police - 0.00%\n",
      "  3) get_well - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) bed - 100.00%\n",
      "  2) police - 0.00%\n",
      "  3) get_well - 0.00%\n",
      "  4) sibling - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) tomorrow - 99.99%\n",
      "  2) who - 0.00%\n",
      "  3) goodbye - 0.00%\n",
      "  4) same - 0.00%\n",
      "  5) wait - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) tomorrow - 100.00%\n",
      "  2) who - 0.00%\n",
      "  3) goodbye - 0.00%\n",
      "  4) wait - 0.00%\n",
      "  5) bring - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) bring - 53.55%\n",
      "  2) police - 23.32%\n",
      "  3) full - 11.10%\n",
      "  4) tomorrow - 8.68%\n",
      "  5) time - 1.40%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) bring - 53.55%\n",
      "  2) police - 23.32%\n",
      "  3) full - 11.10%\n",
      "  4) tomorrow - 8.68%\n",
      "  5) time - 1.40%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) bring - 53.55%\n",
      "  2) police - 23.32%\n",
      "  3) full - 11.10%\n",
      "  4) tomorrow - 8.68%\n",
      "  5) time - 1.40%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) bring - 53.55%\n",
      "  2) police - 23.32%\n",
      "  3) full - 11.10%\n",
      "  4) tomorrow - 8.68%\n",
      "  5) time - 1.40%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) bring - 53.55%\n",
      "  2) police - 23.32%\n",
      "  3) full - 11.10%\n",
      "  4) tomorrow - 8.68%\n",
      "  5) time - 1.40%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) why - 100.00%\n",
      "  2) time - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) glove - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) why - 100.00%\n",
      "  2) time - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) glove - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) why - 99.99%\n",
      "  2) time - 0.01%\n",
      "  3) forbidden - 0.00%\n",
      "  4) glove - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) why - 57.85%\n",
      "  2) time - 42.11%\n",
      "  3) forbidden - 0.04%\n",
      "  4) glove - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) why - 93.81%\n",
      "  2) time - 6.16%\n",
      "  3) forbidden - 0.03%\n",
      "  4) glove - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) time - 99.85%\n",
      "  2) why - 0.11%\n",
      "  3) forbidden - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) time - 99.85%\n",
      "  2) why - 0.11%\n",
      "  3) forbidden - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) time - 99.85%\n",
      "  2) why - 0.11%\n",
      "  3) forbidden - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) time - 99.85%\n",
      "  2) why - 0.11%\n",
      "  3) forbidden - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) time - 99.85%\n",
      "  2) why - 0.11%\n",
      "  3) forbidden - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) friend - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) get_well - 47.87%\n",
      "  2) forbidden - 40.40%\n",
      "  3) bring - 9.40%\n",
      "  4) why - 1.44%\n",
      "  5) same - 0.39%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) get_well - 47.87%\n",
      "  2) forbidden - 40.40%\n",
      "  3) bring - 9.40%\n",
      "  4) why - 1.44%\n",
      "  5) same - 0.39%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) get_well - 47.87%\n",
      "  2) forbidden - 40.40%\n",
      "  3) bring - 9.40%\n",
      "  4) why - 1.44%\n",
      "  5) same - 0.39%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) get_well - 47.87%\n",
      "  2) forbidden - 40.40%\n",
      "  3) bring - 9.40%\n",
      "  4) why - 1.44%\n",
      "  5) same - 0.39%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) bring - 99.99%\n",
      "  2) single - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) get_well - 0.00%\n",
      "  5) tomorrow - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) same - 91.48%\n",
      "  2) where - 5.75%\n",
      "  3) belt - 2.06%\n",
      "  4) always - 0.56%\n",
      "  5) accident - 0.07%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) same - 91.48%\n",
      "  2) where - 5.75%\n",
      "  3) belt - 2.06%\n",
      "  4) always - 0.56%\n",
      "  5) accident - 0.07%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) same - 91.48%\n",
      "  2) where - 5.75%\n",
      "  3) belt - 2.06%\n",
      "  4) always - 0.56%\n",
      "  5) accident - 0.07%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) same - 91.48%\n",
      "  2) where - 5.75%\n",
      "  3) belt - 2.06%\n",
      "  4) always - 0.56%\n",
      "  5) accident - 0.07%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) same - 91.48%\n",
      "  2) where - 5.75%\n",
      "  3) belt - 2.06%\n",
      "  4) always - 0.56%\n",
      "  5) accident - 0.07%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) get_well - 100.00%\n",
      "  2) belt - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) thanks - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) get_well - 100.00%\n",
      "  2) belt - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) thanks - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) get_well - 100.00%\n",
      "  2) belt - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) thanks - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) get_well - 99.99%\n",
      "  2) belt - 0.01%\n",
      "  3) why - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) why - 99.97%\n",
      "  2) forbidden - 0.03%\n",
      "  3) always - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:07,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) bed - 97.08%\n",
      "  2) police - 2.16%\n",
      "  3) hurry - 0.29%\n",
      "  4) single - 0.20%\n",
      "  5) where - 0.09%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) where - 89.95%\n",
      "  2) bed - 5.65%\n",
      "  3) police - 3.11%\n",
      "  4) accident - 0.41%\n",
      "  5) hurry - 0.30%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) where - 90.90%\n",
      "  2) bed - 5.37%\n",
      "  3) police - 2.46%\n",
      "  4) accident - 0.35%\n",
      "  5) hurry - 0.27%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) where - 89.50%\n",
      "  2) police - 8.49%\n",
      "  3) bed - 1.06%\n",
      "  4) hurry - 0.25%\n",
      "  5) forbidden - 0.25%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) tomorrow - 99.50%\n",
      "  2) single - 0.41%\n",
      "  3) same - 0.05%\n",
      "  4) full - 0.03%\n",
      "  5) wait - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:08,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) friend - 96.79%\n",
      "  2) full - 1.64%\n",
      "  3) police - 0.83%\n",
      "  4) bed - 0.42%\n",
      "  5) hurry - 0.18%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) friend - 96.79%\n",
      "  2) full - 1.64%\n",
      "  3) police - 0.83%\n",
      "  4) bed - 0.42%\n",
      "  5) hurry - 0.18%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) friend - 96.79%\n",
      "  2) full - 1.64%\n",
      "  3) police - 0.83%\n",
      "  4) bed - 0.42%\n",
      "  5) hurry - 0.18%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) friend - 96.79%\n",
      "  2) full - 1.64%\n",
      "  3) police - 0.83%\n",
      "  4) bed - 0.42%\n",
      "  5) hurry - 0.18%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) friend - 96.79%\n",
      "  2) full - 1.64%\n",
      "  3) police - 0.83%\n",
      "  4) bed - 0.42%\n",
      "  5) hurry - 0.18%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:09,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) tomorrow - 92.34%\n",
      "  2) single - 5.18%\n",
      "  3) full - 2.29%\n",
      "  4) wait - 0.06%\n",
      "  5) same - 0.04%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) tomorrow - 92.34%\n",
      "  2) single - 5.18%\n",
      "  3) full - 2.29%\n",
      "  4) wait - 0.06%\n",
      "  5) same - 0.04%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) tomorrow - 89.20%\n",
      "  2) single - 9.02%\n",
      "  3) full - 1.63%\n",
      "  4) wait - 0.08%\n",
      "  5) same - 0.04%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) single - 57.66%\n",
      "  2) tomorrow - 41.49%\n",
      "  3) full - 0.77%\n",
      "  4) wait - 0.05%\n",
      "  5) same - 0.01%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) single - 64.18%\n",
      "  2) tomorrow - 35.05%\n",
      "  3) full - 0.69%\n",
      "  4) wait - 0.05%\n",
      "  5) same - 0.01%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 208/208 [00:16<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [95, 96, 142, 143, 144, 207]\n",
      "[95, 142, 207]\n",
      "torch.Size([208, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) belt - 97.42%\n",
      "  2) get_well - 1.24%\n",
      "  3) where - 0.48%\n",
      "  4) accident - 0.37%\n",
      "  5) why - 0.25%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) belt - 99.68%\n",
      "  2) where - 0.12%\n",
      "  3) thanks - 0.06%\n",
      "  4) accident - 0.05%\n",
      "  5) get_well - 0.05%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) hurry - 96.04%\n",
      "  2) why - 3.20%\n",
      "  3) police - 0.71%\n",
      "  4) apologize - 0.04%\n",
      "  5) accident - 0.01%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) police - 98.33%\n",
      "  2) why - 1.15%\n",
      "  3) hurry - 0.49%\n",
      "  4) apologize - 0.02%\n",
      "  5) thanks - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) police - 99.58%\n",
      "  2) why - 0.40%\n",
      "  3) hurry - 0.01%\n",
      "  4) friend - 0.00%\n",
      "  5) apologize - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) belt - 99.94%\n",
      "  2) get_well - 0.02%\n",
      "  3) accident - 0.02%\n",
      "  4) where - 0.01%\n",
      "  5) thanks - 0.01%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) belt - 99.80%\n",
      "  2) where - 0.08%\n",
      "  3) accident - 0.06%\n",
      "  4) get_well - 0.03%\n",
      "  5) thanks - 0.01%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) belt - 99.98%\n",
      "  2) get_well - 0.01%\n",
      "  3) accident - 0.00%\n",
      "  4) where - 0.00%\n",
      "  5) thanks - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) belt - 100.00%\n",
      "  2) thanks - 0.00%\n",
      "  3) accident - 0.00%\n",
      "  4) apologize - 0.00%\n",
      "  5) wait - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) belt - 99.99%\n",
      "  2) where - 0.01%\n",
      "  3) thanks - 0.00%\n",
      "  4) wait - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) who - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) breakfast - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) bring - 0.00%\n",
      "  4) same - 0.00%\n",
      "  5) who - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) good - 100.00%\n",
      "  2) tomorrow - 0.00%\n",
      "  3) thanks - 0.00%\n",
      "  4) who - 0.00%\n",
      "  5) same - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) goodbye - 99.87%\n",
      "  2) bring - 0.13%\n",
      "  3) thanks - 0.00%\n",
      "  4) get_well - 0.00%\n",
      "  5) belt - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) belt - 85.95%\n",
      "  2) goodbye - 13.95%\n",
      "  3) thanks - 0.08%\n",
      "  4) get_well - 0.01%\n",
      "  5) always - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 140/140 [00:11<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [57, 58, 59, 60, 139]\n",
      "[57, 139]\n",
      "torch.Size([140, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) forbidden - 100.00%\n",
      "  2) where - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) forbidden - 100.00%\n",
      "  2) where - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) forbidden - 100.00%\n",
      "  2) where - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) accident - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) forbidden - 100.00%\n",
      "  2) where - 0.00%\n",
      "  3) accident - 0.00%\n",
      "  4) why - 0.00%\n",
      "  5) bring - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) forbidden - 100.00%\n",
      "  2) where - 0.00%\n",
      "  3) why - 0.00%\n",
      "  4) accident - 0.00%\n",
      "  5) same - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) police - 100.00%\n",
      "  2) forbidden - 0.00%\n",
      "  3) full - 0.00%\n",
      "  4) thanks - 0.00%\n",
      "  5) time - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) tomorrow - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) tomorrow - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) bring - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) police - 100.00%\n",
      "  2) full - 0.00%\n",
      "  3) forbidden - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) time - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 76/76 [00:06<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [75]\n",
      "[75]\n",
      "torch.Size([76, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) glove - 72.33%\n",
      "  2) friend - 27.64%\n",
      "  3) time - 0.02%\n",
      "  4) why - 0.00%\n",
      "  5) tomorrow - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) friend - 99.99%\n",
      "  2) glove - 0.01%\n",
      "  3) time - 0.00%\n",
      "  4) tomorrow - 0.00%\n",
      "  5) bed - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) friend - 100.00%\n",
      "  2) bed - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) hurry - 0.00%\n",
      "  5) good - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) friend - 100.00%\n",
      "  2) bed - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) hurry - 0.00%\n",
      "  5) good - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) friend - 93.82%\n",
      "  2) glove - 6.13%\n",
      "  3) good - 0.02%\n",
      "  4) time - 0.01%\n",
      "  5) bed - 0.01%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 129/129 [00:11<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [128]\n",
      "[128]\n",
      "torch.Size([129, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) glove - 98.51%\n",
      "  2) time - 0.59%\n",
      "  3) why - 0.38%\n",
      "  4) hurry - 0.30%\n",
      "  5) apologize - 0.08%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) glove - 99.66%\n",
      "  2) time - 0.31%\n",
      "  3) friend - 0.02%\n",
      "  4) why - 0.01%\n",
      "  5) hurry - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) glove - 99.86%\n",
      "  2) friend - 0.14%\n",
      "  3) time - 0.00%\n",
      "  4) why - 0.00%\n",
      "  5) breakfast - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) glove - 100.00%\n",
      "  2) forbidden - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) good - 0.00%\n",
      "  5) who - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) glove - 100.00%\n",
      "  2) forbidden - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) good - 0.00%\n",
      "  5) who - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 131/131 [00:11<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarks extracted successfully \n",
      "mediapipe features extracted successfully \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "boundaries is:  [130]\n",
      "[130]\n",
      "torch.Size([131, 3, 512, 512])\n",
      "generated sequences successfully \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for sample 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1) friend - 99.92%\n",
      "  2) time - 0.05%\n",
      "  3) why - 0.03%\n",
      "  4) hurry - 0.00%\n",
      "  5) bed - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 1:\n",
      "  1) why - 95.20%\n",
      "  2) friend - 3.38%\n",
      "  3) police - 1.10%\n",
      "  4) belt - 0.16%\n",
      "  5) time - 0.09%\n",
      "\n",
      "Top-5 predictions for sample 2:\n",
      "  1) full - 96.56%\n",
      "  2) police - 3.32%\n",
      "  3) tomorrow - 0.03%\n",
      "  4) time - 0.02%\n",
      "  5) belt - 0.02%\n",
      "\n",
      "Top-5 predictions for sample 3:\n",
      "  1) good - 100.00%\n",
      "  2) bring - 0.00%\n",
      "  3) single - 0.00%\n",
      "  4) time - 0.00%\n",
      "  5) tomorrow - 0.00%\n",
      "\n",
      "Top-5 predictions for sample 4:\n",
      "  1) good - 100.00%\n",
      "  2) single - 0.00%\n",
      "  3) time - 0.00%\n",
      "  4) bring - 0.00%\n",
      "  5) full - 0.00%\n",
      "predictions successfully \n",
      "INFO:     154.189.139.66:0 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uploadManager = UploadManager(\n",
    "        prediciton_dictanory={0: 'accident',\n",
    "  1: 'always',\n",
    "  2: 'apologize',\n",
    "  3: 'bed',\n",
    "  4: 'belt',\n",
    "  5: 'breakfast',\n",
    "  6: 'bring',\n",
    "  7: 'forbidden',\n",
    "  8: 'friend',\n",
    "  9: 'full',\n",
    "  10: 'get_well',\n",
    "  11: 'glove',\n",
    "  12: 'good',\n",
    "  13: 'goodbye',\n",
    "  14: 'hurry',\n",
    "  15: 'police',\n",
    "  16: 'same',\n",
    "  17: 'sibling',\n",
    "  18: 'single',\n",
    "  19: 'thanks',\n",
    "  20: 'time',\n",
    "  21: 'tomorrow',\n",
    "  22: 'wait',\n",
    "  23: 'where',\n",
    "  24: 'who',\n",
    "  25: 'why'},\n",
    "        model_weights_path=model_path,\n",
    "        boundary_model_weights_path= boundaryModelPath,\n",
    "        num_partitions=5,\n",
    "        extractOneFromEach= 2,\n",
    "        intersectionRatio = 0.9,\n",
    "        predict_after = 60,\n",
    "        target_size=(512,512),\n",
    "        debuging=False,\n",
    "        debuging_landmarks=False,\n",
    "        debuging_classifier = False\n",
    "            )\n",
    "@app.post(\"/check\")\n",
    "async def upload_frame(frame: UploadFile = File(...)):\n",
    "    print(\"im in: \")\n",
    "    contents = await frame.read()\n",
    "    image = Image.open(io.BytesIO(contents))\n",
    "    print(\"this is the image: \",image)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    checker = uploadManager.checkLandmarks(image)\n",
    "\n",
    "\n",
    "    return JSONResponse(content=checker, status_code=200)\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload_video(video: UploadFile = File(...)):\n",
    "\n",
    "    os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "    video_path = f\"videos/{video.filename}\"\n",
    "    with open(video_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(video.file, buffer)\n",
    "\n",
    "    frames =uploadManager.extractFrames(video_path)\n",
    "    # for frame in frames:\n",
    "    #   plt.imshow(frame)\n",
    "    #   plt.show()\n",
    "    prediction = uploadManager.predict(frames)\n",
    "\n",
    "    return {\"status\": \"success\", \"glosses\": prediction}\n",
    "\n",
    "@app.post(\"/upload_desktop\")\n",
    "async def upload_video(video: UploadFile = File(...)):\n",
    "\n",
    "    os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "    video_path = f\"videos/{video.filename}\"\n",
    "    with open(video_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(video.file, buffer)\n",
    "\n",
    "    frames =uploadManager.extractFrames(video_path,video=True)\n",
    "    # for frame in frames:\n",
    "    #   plt.imshow(frame)\n",
    "    #   plt.show()\n",
    "    prediction = uploadManager.predict(frames)\n",
    "\n",
    "    return {\"status\": \"success\", \"glosses\": prediction}\n",
    "\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vkk6SvWBx98N",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7612157,
     "sourceId": 12092254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7612010,
     "sourceId": 12099723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7650657,
     "sourceId": 12147354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7723346,
     "sourceId": 12257354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7739111,
     "sourceId": 12282725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7739165,
     "sourceId": 12282765,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7740921,
     "sourceId": 12282993,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7744666,
     "sourceId": 12288578,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7747614,
     "sourceId": 12292652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7750145,
     "sourceId": 12296392,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7750619,
     "isSourceIdPinned": true,
     "sourceId": 12297110,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7796147,
     "isSourceIdPinned": false,
     "sourceId": 12365004,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 390369,
     "modelInstanceId": 369497,
     "sourceId": 455651,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
